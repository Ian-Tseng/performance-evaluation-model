# -*- coding: utf-8 -*-
"""perf_eva_single_val_nf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VyplfYi1v992KsY2HE4nhKVdbsKL7xPu
"""

from google.colab import drive
drive.mount('/content/drive')
#drive.mount('/content/drive', force_remount= True)

root_dir= '/content/drive/MyDrive/Colab_drive/perf_eva/'

from tensorflow.python.ops.numpy_ops import np_config
from keras import backend as K
from scipy.spatial import distance
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession
from tensorflow.keras import layers
from tensorflow import keras
from scipy.stats import pearsonr
from scipy import stats
from IPython import get_ipython
from threading import Thread
from numba import cuda


#import tensorflow_probability as tfp
import statistics
import gc
import csv
import ast
import random
import math
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import sys
import os
import time
import glob
import pandas as pd
import pickle

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.2
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
np_config.enable_numpy_behavior()

mirrored_strategy = tf.distribute.MirroredStrategy()
gpus = tf.config.list_physical_devices('GPU')
if gpus:
  # Create 2 virtual GPUs with 1GB memory each
  #  for gpu in gpus:
  #      tf.config.experimental.set_memory_growth(gpu, True)
    try:
        tf.config.set_logical_device_configuration(
        gpus[0],
        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),
         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])
        logical_gpus = tf.config.list_logical_devices('GPU')
        print(len(gpus), "Physical GPU,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
    # Virtual devices must be set before GPUs have been initialized
        print(e)

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print (tf.sysconfig.get_build_info() )

##### # The requirements to use the cuDNN implementation are:

# activation == tanh
# recurrent_activation == sigmoid
# recurrent_dropout == 0
# unroll is False
# use_bias is True
# Inputs, if use masking, are strictly right-padded.
# Eager execution is enabled in the outermost context.


def get_cross_train_rule_data_and_validation_data(dir):
    total_data_list= []

    session_list= []
    with open(dir, 'r+', newline='') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')
        row_count= 0
        for row in reader:
            row_list= []
            for col in row:
                if not col:
                    break

                if ast.literal_eval(col)== True:
                    data_list= ast.literal_eval(col)
                    if not data_list in total_data_list:
                        row_list.append(data_list)
                else:
                    data_int= ast.literal_eval(col)
                    if not data_int in total_data_list:
                        row_list.append(data_int)

            total_data_list.append(row_list)

            row_count+= 1
            if row_count> 16: # Select rows
                break


        num_of_session= 5    # Number of cross validation session

        for s in range(num_of_session):
            start_val_data= int(len(total_data_list)/ num_of_session* s)
            num_of_val_data= int(row_count* 1/ 3)
            train_rule_data_list= []
            validation_data_list= []
            new_data=  start_val_data+ num_of_val_data
            for row_index in range(len(total_data_list)):
                val_data_count= 0
                if row_index in range(start_val_data, start_val_data+ num_of_val_data):
                    validation_data_list.append(total_data_list[row_index])
                else:
                    train_rule_data_list.append(total_data_list[row_index])
            session_list.append([train_rule_data_list, validation_data_list])

    return session_list

def get_rand_train_rule_data_and_validation_data(dir):
    total_data_list= []

    session_list= []
    with open(dir, 'r+', newline='') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')
        row_count= 0
        for row in reader:
            row_list= []
            for col in row:
                if not col:
                    break

                if ast.literal_eval(col)== True:
                    data_list= ast.literal_eval(col)
                    if not data_list in total_data_list:
                        row_list.append(data_list)
                else:
                    data_int= ast.literal_eval(col)
                    if not data_int in total_data_list:
                        row_list.append(data_int)

            total_data_list.append(row_list)

            row_count+= 1
            if row_count> 16: # Select rows
                break

        num_of_train_data= int(row_count* 2/ 3)
        num_of_val_data= int(len(total_data_list)- num_of_train_data)
        session_num= 2    # cross validation


        rand_sel_record= []

        for session in range(session_num):
            total_data_list_copy= total_data_list.copy()
            validation_data_list= []
            for num in range(num_of_val_data):
                rand_sel= random.choice(total_data_list_copy)
                while rand_sel in rand_sel_record:
                    rand_sel= random.choice(total_data_list_copy)
                total_data_list_copy.remove(rand_sel)
                validation_data_list.append(rand_sel)
                rand_sel_record.append(rand_sel)
            train_rule_data_list= total_data_list_copy
            session_list.append([train_rule_data_list, validation_data_list])

    return session_list

def get_train_and_val_data_by_classes(dir_list:list):
    data_with_classes_list=[]
    session_list= []
    for dir in dir_list:
        print ('dir', dir)
        total_data_list= []
        with open(dir, 'r+', newline='' , encoding='utf-8-sig') as csvfile:
            reader = csv.reader(csvfile, delimiter=',')

            for row in reader:

                row_list= []

                if len(row[2])== 0:
                    continue

                conn_last_index= [i for i, c in enumerate(row) if '->' in c ][-1]
                for i, col in enumerate(row):
                    if i == conn_last_index+ 2:
                        break


                    if ast.literal_eval(col)== True:
                        data_list= ast.literal_eval(col)

                        if not data_list in total_data_list:
                            row_list.append(data_list)
                    else:
                        data_int= ast.literal_eval(col)
                        if not data_int in total_data_list:
                            row_list.append(data_int)

                if not row_list:
                    continue


                total_data_list.append(row_list)

        data_with_classes_list.append(total_data_list)


    sessions_with_classes= []
    for total_data_list in data_with_classes_list:
      #  print ('total_data_list' , np.array(total_data_list).shape)

        # Set param
        num_of_train_data= 20 #
        split_rate= 1
        num_of_session= 3   # cross validation
        num_of_val_data= 15

        for session in range(num_of_session):
            validation_data_list= []
            total_data_list_copy= total_data_list.copy()[1:]


            if session== 0:
                num_of_train_data= 20
            elif session== 1:
                num_of_train_data= 30
            elif session==2:
                num_of_train_data= 40
            elif session==3:
                num_of_val_data= len(total_data_list_copy)-1

            # Val data
            if num_of_val_data== None:
                num_of_val_data= int(len(total_data_list_copy) *split_rate)
            val_data_list= [i for i in reversed(total_data_list_copy)][:num_of_val_data]


            # Train data

            train_data_list=  total_data_list_copy[:num_of_train_data-1]+ [total_data_list[0]]

            [total_data_list_copy.remove(total_data_list_copy[0]) for i in range(len(train_data_list)) ]


            session_list.append([train_data_list, val_data_list])


            print (f'session index {session}  length of total data {len(total_data_list)}' )
            print (f'session index {session}  length of train data {len(train_data_list)}' )
            print (f'session index {session}  length of val data {len(val_data_list)}' )
        sessions_with_classes.append(session_list)


    return sessions_with_classes


def get_variance_data(session_list:list, session_index:int):
    for data in session_list[session_index][0]:
        col_index_list=  [i for i, c in enumerate(data) if isinstance(c, list) and '->' in c ]
        for i in col_index_list:
                if not  f'{data[i][0]}_{data[i][-1]}' in variance_dict:
                    variance_dict[f'{data[i][0]}_{data[i][-1]}']=[float(data[i+1])]
                else:
                    if not float(data[i+1]) in variance_dict[f'{data[i][0]}_{data[i][-1]}']:
                        variance_dict[f'{data[i][0]}_{data[i][-1]}'].append(float(data[i+1]))

def get_train_and_val_data_by_split(dir:str):
    total_data_list= []
    session_list= []
    data_variance_dict= {}
    with open(dir, 'r+', newline='' , encoding='utf-8-sig') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')

        for row in reader:

            row_list= []

            if len(row[2])== 0:
                continue

            conn_last_index= [i for i, c in enumerate(row) if '->' in c ][-1]
            for i, col in enumerate(row):
                if i == conn_last_index+ 2:
                    break

                if ast.literal_eval(col)== True:
                    data_list= ast.literal_eval(col)

                    if not data_list in total_data_list:
                        row_list.append(data_list)
                else:
                    data_int= ast.literal_eval(col)
                    if not data_int in total_data_list:
                        row_list.append(data_int)

            total_data_list.append(row_list)

        # Add expected value to variance dict
            #col_index_list=  [i for i, c in enumerate(row) if '->' in c ]
            #for i in col_index_list:
            #    if not  f'{row_list[i][0]}_{row_list[i][-1]}' in variance_dict:
            #        variance_dict[f'{row_list[i][0]}_{row_list[i][-1]}']=[float(row_list[i+1])]
            #    else:
            #        if not float(row_list[i+1]) in variance_dict[f'{row_list[i][0]}_{row_list[i][-1]}']:
            #            variance_dict[f'{row_list[i][0]}_{row_list[i][-1]}'].append(float(row_list[i+1]))
        #for data in total_data_list:

        #    col_index_list=  [i for i, c in enumerate(data) if isinstance(c, list) and '->' in c  ]
        #    data_variance= np.mean([statistics.variance(variance_dict[f'{data[c][0]}_{data[c][-1]}']) for c in col_index_list if
        #                            f'{data[c][0]}_{data[c][-1]}' in variance_dict and len(variance_dict[f'{data[c][0]}_{data[c][-1]}'])> 1]  )

        #    data_variance_dict[str(data[0])]= data_variance


        # Get sum of variance of all answers

       # print ('data_variance_dict', data_variance_dict)



        # Set param
        num_of_train_data= 20 #
        split_rate= 1
        num_of_session= 4  # cross validation
        num_of_val_data= 15

     #   total_data_list_copy= sorted([[data_variance_dict[str(i[0])], i] for i in total_data_list.copy()[1:]], key=lambda a: a[0])
     #   print ('total_data_list_copy', [i[0] for i in total_data_list_copy])
     #   total_data_list_copy= [i[1] for i in total_data_list_copy]
        total_data_list_copy= total_data_list.copy()[1:]
        random.shuffle(total_data_list_copy)
        val_data_list= [i for i in reversed(total_data_list_copy)][:num_of_val_data]
        for session in range(num_of_session):
            validation_data_list= []

            if session== 0:
                num_of_train_data= 20
            elif session== 1:
                num_of_train_data= 30
            elif session== 2:
                num_of_train_data= 40
            elif session== 3:
                num_of_train_data= len(total_data_list)-1
                val_data_list= total_data_list.copy()[1:]


            # Val data
            if num_of_val_data== None:
                num_of_val_data= int(len(total_data_list_copy) *split_rate)



            # Train data

            train_data_list=  total_data_list_copy[:num_of_train_data-1]+ [total_data_list[0]]

          #  [total_data_list_copy.remove(total_data_list_copy[0]) for i in range(len(train_data_list)) ]


            session_list.append([train_data_list, val_data_list])


            print ('total data len', len(total_data_list))
            print ('train data len', len(train_data_list))
            print ('validation data len', len(val_data_list))


    return session_list



def get_train_and_val_data_for_general_score(dir:str):
    total_data_list= []
    session_list= []
    with open(dir, 'r+', newline='' , encoding='utf-8-sig') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')

        for row in reader:

            row_list= []

            if len(row[2])== 0:
                continue

            conn_last_index= [i for i, c in enumerate(row) if '->' in c][-1]
            for i, col in enumerate(row):
               # if i == conn_last_index+ 2:
               #     break
                if col== 'general_score':
                    continue

                if ast.literal_eval(col)== True:
                    data_list= ast.literal_eval(col)

                    if not data_list in total_data_list:
                        row_list.append(data_list)
                else:
                    data_int= ast.literal_eval(col)
                    if not data_int in total_data_list:
                        row_list.append(data_int)


            total_data_list.append(row_list)


        # Set param
        num_of_train_data= 20 #
        split_rate= 1
        num_of_session= 3   # cross validation
        num_of_val_data= 15

        for session in range(num_of_session):
            validation_data_list= []
            total_data_list_copy= total_data_list.copy()[1:]
            random.shuffle(total_data_list_copy)


            if session== 0:
                num_of_train_data= 20
            elif session== 1:
                num_of_train_data= 30
            elif session== 2:
                num_of_train_data= 40
           # elif session==2:
           #     random.shuffle(total_data_list_copy)

            # Val data
            if num_of_val_data== None:
                num_of_val_data= int(len(total_data_list_copy) *split_rate)
            val_data_list= [i for i in reversed(total_data_list_copy)][:num_of_val_data]


            # Train data

            train_data_list=  total_data_list_copy[:num_of_train_data-1]+ [total_data_list[0]]

            [total_data_list_copy.remove(total_data_list_copy[0]) for i in range(len(train_data_list)) ]


            session_list.append([train_data_list, val_data_list])


            print ('total data len', len(total_data_list))
            print ('train data len', len(train_data_list))
            print ('validation data len', len(val_data_list))


    return session_list




def get_target_train_rule_data_and_validation_data_test_data(dir):
    total_data_list= []
    target_row= list(range(19, 24))  # 19- 24
    target_train_data_index_list= [25]
    target_val_data_for_histroy_record= [1, 3, 5, 7, 9, 10, 12]
    session_list= []
    specific_validation_data_list= []
    train_data_list= []
    with open(dir, 'r+', newline='' , encoding='utf-8-sig') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')
        row_count= 0

        for row in reader:

            row_list= []

            if len(row[2])== 0:
                continue

            conn_last_index= [i for i, c in enumerate(row) if '->' in c ][-1]
            for i, col in enumerate(row):
                if i == conn_last_index+ 2:
                    break
              #  if not col:
              #      break


                if ast.literal_eval(col)== True:
                    data_list= ast.literal_eval(col)

                    if not data_list in total_data_list:
                        row_list.append(data_list)
                else:
                    data_int= ast.literal_eval(col)
                    if not data_int in total_data_list:
                        row_list.append(data_int)


            total_data_list.append(row_list)
            if row_count in target_row:
                specific_validation_data_list.append(row_list)
            else:
                train_data_list.append(row_list)

            row_count+= 1


        num_of_train_data= 50
        num_of_val_data_rate= 0.3
        num_of_val_data= int(num_of_train_data* num_of_val_data_rate)
        num_of_session= 3    # cross validation



        for session in range(num_of_session):
            validation_data_list= []
            train_data_list_copy= train_data_list.copy()
            number_of_val_count= 0


            if session== 0:
                while number_of_val_count< num_of_val_data:
                    rand_sel= random.choice(train_data_list_copy)

                    validation_data_list.append(rand_sel)
                    number_of_val_count+= 1
                train_data_list_copy= train_data_list_copy[: num_of_train_data]
                #while len(train_data_list_copy)> num_of_train_data:
                #    rand_sel= random.choice(train_data_list_copy)
                #    train_data_index= train_data_list.index(rand_sel)
                #    if train_data_index in target_train_data_index_list:
                #        continue
                #    train_data_list_copy.remove(rand_sel)

                target_val_data_list= validation_data_list
            if session== 1:
                target_val_data_list= specific_validation_data_list


            if session== 2:
                for i in target_val_data_for_histroy_record:
                    target_sel= train_data_list_copy[i]

                   # specific_validation_data_list.remove(target_sel)
                    number_of_val_count+= 1
                    validation_data_list.append(target_sel)
                target_val_data_list= validation_data_list

            session_list.append([train_data_list_copy, target_val_data_list])


            print ('total data len', len(total_data_list))
            print ('train data len', len(train_data_list_copy))
            print ('validation data len', len(target_val_data_list))
            print ('specific_validation_data_list len', len(specific_validation_data_list))


    return session_list

def save_train_and_val_data(dir:str, data:list):
    with open(dir, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(data)







def get_layer_output_value(model, input_data_list_arr):
    inp = model.input
    outputs = [layer.output for layer in model.layers]
    functors = [K.function([inp], [out]) for out in outputs]
    layer_outs = [func([input_data_list_arr]) for func in functors]






def sort_to_combination(sort_list):
    combination_list= []
    for sub_component_index in range(len(sort_list)):
        if sub_component_index== len(sort_list)- 1:
            break
        sub_component= [sort_list[sub_component_index], sort_list[sub_component_index+ 1]]
        combination_list.append(sub_component)
    return combination_list

def data_dist_info(component_data, train_rule_data_sort, expected_data_sort, outside_segment_data_sort):
    data_dist_list= []
    all_segments= expected_data_sort+ outside_segment_data_sort
    outside_segment_info= len(all_segments)+ 1
    component_position_in_train_rule_data_list= []
    component_position_in_expected_data_list= []
    component_position_in_outside_data_list= []
    if component_data in expected_data_sort:
        for segment_index in expected_data_sort:
      #  if not segment_index in train_rule_data_sort or not component_data in train_rule_data_sort: # The segments not in expected data. The dist== length of expected data + 1
      #      data_dist=  len(all_segments)+ 1
      #      data_dist_list.append(data_dist)
      #      continue

            component_data_index= train_rule_data_sort.index(component_data) # Component position
            expected_data_index= expected_data_sort.index(segment_index) # Each segments position
            data_dist= abs(expected_data_index- component_data_index) # The distance of component data position to expected data position. If the component data not in expected data dist info == 0.
            data_dist_list.append(data_dist)
            component_position_in_train_rule_data_list.append(component_data_index)
            component_position_in_expected_data_list.append(expected_data_index)
        for outside_segment_index in outside_segment_data_sort:
            data_dist_list.append(outside_segment_info)

        dist_val= distance.minkowski(component_position_in_train_rule_data_list, component_position_in_expected_data_list, len(component_position_in_train_rule_data_list))
        dist_coding_list= []
        dist_val= list(str(dist_val))
        dist_val.remove('.')
        for dist in dist_val:
            dist_coding_list.append(int(dist))
        while len(dist_coding_list)< 16:
            dist_coding_list.append(0)
        while len(dist_coding_list)> 16:
            dist_coding_list.pop(-1)
        dist_coding_list= list(np.array(dist_coding_list)+ len(expected_data_sort)*2)

    else:
        for segment_index in expected_data_sort:
            data_dist_list.append(outside_segment_info)

        for outside_segment_index in outside_segment_data_sort:
            component_data_index= train_rule_data_sort.index(component_data)
            outside_data_index= outside_segment_data_sort.index(outside_segment_index)

            component_position_in_train_rule_data_list.append(component_data_index)
            component_position_in_outside_data_list.append(outside_data_index)

            data_dist= abs(outside_data_index- component_data_index)
            data_dist_list.append(data_dist)

        dist_val= distance.minkowski(component_position_in_train_rule_data_list, component_position_in_outside_data_list, len(component_position_in_train_rule_data_list))
        dist_coding_list= []
        dist_val= list(str(dist_val))
        dist_val.remove('.')
        for dist in dist_val:
            dist_coding_list.append(int(dist))
        while len(dist_coding_list)< 16:
            dist_coding_list.append(0)
        while len(dist_coding_list)> 16:
            dist_coding_list.pop(-1)
        dist_coding_list= list(np.array(dist_coding_list)+ len(expected_data_sort)*2)

    return dist_coding_list




def data_conn_info_0(component_data, train_rule_data_sort, expected_data_sort, outside_segment_data_sort, all_segments_dict):
    data_conn_info_list= []
    data_component_info_list= []
    component_data_index= train_rule_data_sort.index(component_data)
    conn= 1
    not_conn= 0


    for segment in train_rule_data_sort:
        conn_info= None
        target_ssegmentegment= 0

        in_expected_data_value, segment_index_in_all_segments_dict= int(all_segments_dict.get(segment).split('_')[0]), int(all_segments_dict.get(segment).split('_')[1])

        data_conn_info_list.append(segment_index_in_all_segments_dict) # In specific index
        if segment in expected_data_sort:
            segment_categ= expected_data_sort.index(segment)


            if segment_categ== len(expected_data_sort)-1:  # If segemnt is the last thing in expected data, there were no connect thing exist.
                conn_info= not_conn



            else:
                conn_index_in_train_data= segment+ 1 # Get connect segment index of current segment in train data
                conn_index_in_expected_data= segment_categ+ 1  # Get connect segment index in expected data of current segemnt

                if conn_index_in_train_data== len(train_rule_data_sort):
                    conn_info= not_conn
                if conn_index_in_expected_data== len(train_rule_data_sort):
                    conn_info= not_conn
                    continue
                if conn_index_in_train_data!= len(train_rule_data_sort) and conn_index_in_expected_data!= len(train_rule_data_sort):
                    if train_rule_data_sort[conn_index_in_train_data] ==  expected_data_sort[conn_index_in_expected_data]:
                        conn_info= conn
                    else:
                        conn_info= not_conn

        else:

            conn_info= not_conn

        if segment== component_data:
            target_segment= 1

        data_conn_info_list.append(target_segment)
        data_conn_info_list.append(conn_info)

    in_expected_data_value, component_index= int(all_segments_dict.get(component_data).split('_')[0]), int(all_segments_dict.get(component_data).split('_')[1])



    return data_conn_info_list, data_component_info_list

def graph_features(component_datas:list, train_rule_data_sort:list, expected_data_sort:list, all_segments_dict:dict)-> list:
    correct_conn= correct_ele=2
    weighted_conn= weighted_ele= 1
    conn= exists_ele= 0.5
    not_conn= not_exists_ele= 0

    expected_graph_value_list= []
    actual_graph_value_list= []
    for row in all_segments_dict:
        col_list= []
        for col in all_segments_dict:
            target_val= None
            if row== col:
                target_val= not_conn
            elif not row in expected_data_sort or not col in expected_data_sort:
                target_val= not_conn
            elif expected_data_sort.index(row)+ 1== expected_data_sort.index(col):
               # if [row, col]== component_datas:
                target_val= correct_conn
              #  else:
               #     target_val= weighted_conn
            else:
                target_val= not_conn
            col_list.append(target_val)
        expected_graph_value_list.append(col_list)

    for row in all_segments_dict:
        col_list= []
        for col in all_segments_dict:
            target_val= None
            if row== col:
                target_val= not_conn
            elif not row in train_rule_data_sort or not col in train_rule_data_sort:
                target_val= not_conn
            elif train_rule_data_sort.index(row)+ 1== train_rule_data_sort.index(col):
                if [row, col]== component_datas:
                    target_val= weighted_conn
                else:
                    target_val= conn
            else:
                target_val= not_conn

            col_list.append(target_val)
        actual_graph_value_list.append(col_list)

    target_value_list= []
    posi_expected_graph_value_list= []
    for row in all_segments_dict:
        col_list= []
        for col in all_segments_dict:
            target_val= None
            if row== col:
                if row in expected_data_sort:

                    target_val= correct_ele
                else:
                    target_val= not_exists_ele
            else:
                target_val= not_exists_ele

            col_list.append(target_val)
        posi_expected_graph_value_list.append(col_list)

    posi_actural_graph_value_list= []
    for row in all_segments_dict:
        col_list= []
        for col in all_segments_dict:
            target_val= None
            if row== col:
                if row in train_rule_data_sort:

                    target_val= exists_ele
                else:
                    target_val= not_exists_ele
            else:
                target_val= not_exists_ele

            col_list.append(target_val)
        posi_actural_graph_value_list.append(col_list)
    target_graph_value_array= np.dot(np.array(expected_graph_value_list), np.array(actual_graph_value_list))
  #  target_graph_value_array= np.array(expected_graph_value_list)+ np.array(actual_graph_value_list)
  #  target_value_list= []
  #  for i, c in enumerate(actual_graph_value_list):
  #      target_value_list.append(i)
  #      target_value_list.append(c)

  #  target_value_list= on_preprocess_one_hot(actual_graph_value_list, 3)


    return target_graph_value_array




def data_conn_info(component_data:int, train_rule_data_sort:list, expected_data_sort:list, all_segments_dict:dict):
    data_conn_info_list= []
    data_component_info_list= []
    component_data_index= train_rule_data_sort.index(component_data)
    conn= 1
    not_conn= 0

    for data_index, segment in enumerate(train_rule_data_sort):
        if data_index== len(train_rule_data_sort)-1:
             continue
        conn_info= None
        conn_segment= train_rule_data_sort[data_index+1]

        in_expected_data_value, segment_index_in_all_segments_dict= int(all_segments_dict.get(conn_segment).split('_')[0]), int(all_segments_dict.get(conn_segment).split('_')[1])
   #     conn_in_expected_data_value, conn_segment_index_in_all_segments_dict= int(all_segments_dict.get(segment).split('_')[0]), int(all_segments_dict.get(segment).split('_')[1])
        data_conn_info_list.append(segment_index_in_all_segments_dict) # In specific index

        if segment in expected_data_sort:
            segment_categ= expected_data_sort.index(segment)


            if segment_categ== len(expected_data_sort)-1:  # If segemnt is the last thing in expected data, there were no connect thing exist.
                conn_info= not_conn

            else:
                conn_index_in_train_data=  data_index+ 1  # Get connect segment index of current segment in train data
                conn_index_in_expected_data= segment_categ+ 1  # Get connect segment index in expected data of current segemnt

                if conn_index_in_train_data>= len(train_rule_data_sort):
                    conn_info= not_conn


                if conn_index_in_expected_data>= len(train_rule_data_sort):
                    conn_info= not_conn


                if conn_index_in_train_data< len(train_rule_data_sort) and conn_index_in_expected_data< len(train_rule_data_sort):
                    if train_rule_data_sort[conn_index_in_train_data] ==  expected_data_sort[conn_index_in_expected_data]:
                        conn_info= conn
                    else:
                        conn_info= not_conn


        else:

            conn_info= not_conn


        data_conn_info_list.append(conn_info)

    in_expected_data_value, component_index= int(all_segments_dict.get(component_data).split('_')[0]), int(all_segments_dict.get(component_data).split('_')[1])

    data_conn_info_list.append(component_index) # In specific index
    data_component_info_list.append(component_index)


    if component_data in expected_data_sort:
        conn_info= None
        component_data_categ= expected_data_sort.index(component_data)
        component_data_in_train_data_categ= train_rule_data_sort.index(component_data)

        if component_data_categ== len(expected_data_sort)-1:
            conn_info= not_conn


        else:


            conn_index_in_train_data=  component_data_in_train_data_categ+ 1  # Get connect segment index of current segment in train data
            conn_index_in_expected_data= component_data_categ+ 1  # Get connect segment index in expected data of current segemnt

            if conn_index_in_train_data>= len(train_rule_data_sort):
                conn_info= not_conn


            if conn_index_in_expected_data>= len(train_rule_data_sort):
                conn_info= not_conn




            if conn_index_in_train_data< len(train_rule_data_sort) and conn_index_in_expected_data< len(train_rule_data_sort):
                    if train_rule_data_sort[conn_index_in_train_data] ==  expected_data_sort[conn_index_in_expected_data]:
                        conn_info= conn
                    else:
                        conn_info= not_conn



    else:
        conn_info= not_conn

    data_conn_info_list.append(conn_info)
    data_component_info_list.append(conn_info)

    return data_conn_info_list, data_component_info_list










def all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort, all_segments_dict):
    all_segments_len= len(all_segments_dict)
    r_posi= 1
    not_r_posi=0
    all_segments_data_posi_info_list= []
    for i, segment in enumerate(train_rule_data_sort):
        posi_cate= i+ all_segments_len
        posi_cate= int(all_segments_dict.get(segment).split('_')[1])

        posi_info= None
        all_segments_data_posi_info_list.append(posi_cate)
        if not segment in expected_data_sort:

            posi_info= not_r_posi

            all_segments_data_posi_info_list.append(posi_info)
            continue

        component_data_index= train_rule_data_sort.index(segment)
        expected_data_index= expected_data_sort.index(segment)
        if component_data_index== expected_data_index:
            posi_info= r_posi
        else:
            posi_info= not_r_posi

        all_segments_data_posi_info_list.append(posi_info)

    return all_segments_data_posi_info_list



def data_posi_info(component_data:int, train_rule_data_sort:list, expected_data_sort:list, all_segments_dict:dict):
    all_segments_len= len(all_segments_dict)

    r_posi=1#all_segments_len+ 1
    not_r_posi=0# all_segments_len+ 2
    posi_info= None

    posi_cate= train_rule_data_sort.index(component_data)+ all_segments_len
    posi_cate= int(all_segments_dict.get(component_data).split('_')[1])

    if not component_data in expected_data_sort:

        posi_info= not_r_posi
    else:
        component_data_index= train_rule_data_sort.index(component_data)
        expected_data_index= expected_data_sort.index(component_data)
        if component_data_index== expected_data_index:
            posi_info= r_posi
        else:
            posi_info= not_r_posi

    return posi_info, posi_cate




def get_input_data_0(expected_data_sort, train_rule_data_list, validation_data_list):
    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    input_data_index_list= []
    validation_input_data_list= []
    validation_data_input_index_list= []
    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)
        for train_rule_data_comb in train_rule_data_comb_list:
            first_component= train_rule_data_comb[0]
            second_component= train_rule_data_comb[1]

            first_comp_data_dist_info_list= data_dist_info(first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_dist_info_list= data_dist_info(second_component, train_rule_data_sort, expected_data_sort)
            first_comp_data_posi_info= data_posi_info(first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_posi_info= data_posi_info(second_component, train_rule_data_sort, expected_data_sort)


            input_data= train_rule_data_sort+ train_rule_data_comb+ first_comp_data_dist_info_list+ second_comp_data_dist_info_list
            input_data.append(first_comp_data_posi_info)
            input_data.append(second_comp_data_posi_info)
            input_data_list.append(input_data)


            input_data_index_list.append([train_rule_data_sort, [first_component, second_component]])
    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
        for validation_data_comb in validation_data_comb_list:
            val_data_first_component= validation_data_comb[0]
            val_data_second_component= validation_data_comb[1]

            first_comp_data_dist_info_list= data_dist_info(val_data_first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_dist_info_list= data_dist_info(val_data_second_component, train_rule_data_sort, expected_data_sort)
            first_comp_data_posi_info= data_posi_info(val_data_first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_posi_info= data_posi_info(val_data_second_component, train_rule_data_sort, expected_data_sort)


            val_input_data= validation_data_sort+ validation_data_comb+ first_comp_data_dist_info_list+ second_comp_data_dist_info_list
            val_input_data.append(first_comp_data_posi_info)
            val_input_data.append(second_comp_data_posi_info)


            validation_input_data_list.append(val_input_data)
            validation_data_input_index_list.append([validation_data_sort, [val_data_first_component, val_data_second_component]])

    return input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list

def get_input_data_for_general_score_0(expected_data_sort:list,
                   outside_segment_data_sort:list,
                   all_segments_dict:dict,
                   train_rule_data_list:list,
                   validation_data_list:list):
    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    validation_input_data_list= []


    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
      #  train_features_data_list= [[(c--5)/(5--5)]  for i, c in enumerate(train_rule_data[:-2]) if i%2== 0 and i!= 0]
        train_features_data_list= [[c]  for i, c in enumerate(train_rule_data[:-2]) if i%2== 0 and i!= 0]
    #    train_features_data_list= [[c_0 if i_0!= i else c  for i_0, c_0 in enumerate(np.zeros(len(train_features_data_list)))  ] for i, c in
    #                               enumerate(train_features_data_list)]




          #  input_data_prepro= on_preprocess_one_hot(input_data, len(input_data))

          #  input_data= input_data_prepro
        input_data_list.append(train_features_data_list)



    for validation_data in validation_data_list:
        new_val_features_data_list= []
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
      #  val_features_data_list= [[(c--5)/(5--5)] for i, c in enumerate(validation_data[:-2]) if i%2== 0 and i!= 0]
        val_features_data_list= [[c] for i, c in enumerate(validation_data[:-2]) if i%2== 0 and i!= 0]


     #   val_features_data_list= [[c_0 if i_0!= i else c  for i_0, c_0 in enumerate(np.zeros(len(val_features_data_list)))  ] for i, c in
     #                            enumerate(val_features_data_list)]
        validation_input_data_list.append(val_features_data_list)


    return np.array(input_data_list), np.array(validation_input_data_list)


def get_input_data_for_general_score(expected_data_sort:list,
                   outside_segment_data_sort:list,
                   all_segments_dict:dict,
                   train_rule_data_list:list,
                   validation_data_list:list):
    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    validation_input_data_list= []


    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)
        train_features_data_list= []
        for train_rule_data_comb in train_rule_data_comb_list:
            first_component= train_rule_data_comb[0]
            second_component= train_rule_data_comb[1]
         #   first_comp_data_dist_info_train_list= data_dist_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
         #   second_comp_data_dist_info_train_list= data_dist_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
         #   data_conn_in_different_level_list= data_conn_in_different_level_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
            first_comp_data_conn_info_train_list, data_component_info_train_list= data_conn_info(first_component,
                                                                                                 train_rule_data_sort,
                                                                                                 expected_data_sort,
                                                                                                 all_segments_dict)
         #   second_comp_data_conn_info_train_list= data_conn_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort,
                                                                          expected_data_sort,
                                                                          outside_segment_data_sort,
                                                                          all_segments_dict)

            first_comp_data_posi_info, first_comp_data_posi_cate= data_posi_info(first_component,
                                                                                 train_rule_data_sort,
                                                                                 expected_data_sort,
                                                                                 all_segments_dict)
            second_comp_data_posi_info, second_comp_data_posi_cate= data_posi_info(second_component,
                                                                                   train_rule_data_sort,
                                                                                   expected_data_sort,
                                                                                   all_segments_dict)




            input_data= first_comp_data_conn_info_train_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_cate, first_comp_data_posi_info]+ [second_comp_data_posi_cate, second_comp_data_posi_info]#+ [first_comp_data_in_expected_data_info]+ [second_comp_data_in_expected_data_info]#++ first_comp_binary_features_list+ second_comp_binary_features_list



          #  input_data_prepro= on_preprocess_one_hot(input_data, len(input_data))

          #  input_data= input_data_prepro
            train_features_data_list+= [input_data]
        input_data_list.append(train_features_data_list)



    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
        val_features_data_list= []
        for validation_data_comb in validation_data_comb_list:
            val_data_first_component= validation_data_comb[0]
            val_data_second_component= validation_data_comb[1]

          #  first_comp_data_dist_info_val_list= data_dist_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
          #  second_comp_data_dist_info_val_list= data_dist_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)

         #   all_data_conn_info_list= all_data_conn_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
          #  val_data_conn_in_different_level_list= data_conn_in_different_level_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
            first_comp_data_conn_info_val_list, data_component_info_val_list= data_conn_info(val_data_first_component,
                                                                                             validation_data_sort,
                                                                                             expected_data_sort,
                                                                                             all_segments_dict)
          #  second_comp_data_conn_info_val_list= data_conn_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort, all_segments_dict)

            first_comp_data_posi_info, first_comp_data_posi_cate= data_posi_info(val_data_first_component,
                                                                                 validation_data_sort,
                                                                                 expected_data_sort,
                                                                                 all_segments_dict)
            second_comp_data_posi_info, second_comp_data_posi_cate= data_posi_info(val_data_second_component,
                                                                                   validation_data_sort,
                                                                                   expected_data_sort,
                                                                                   all_segments_dict)

            val_input_data= first_comp_data_conn_info_val_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_cate, first_comp_data_posi_info]+ [second_comp_data_posi_cate, second_comp_data_posi_info]#+ [first_comp_data_in_expected_data_info]+ [second_comp_data_in_expected_data_info]#+ first_comp_binary_features_list+ second_comp_binary_features_list



         #   val_input_data_prepro= on_preprocess_one_hot(val_input_data, len(val_input_data))
         #   val_input_data= val_input_data_prepro
            val_features_data_list+= [val_input_data]
        validation_input_data_list.append(val_features_data_list)


    return np.array(input_data_list), np.array(validation_input_data_list)

# Directed graph
def get_input_data_with_graph_features(expected_data_sort:list,
                   outside_segment_data_sort:list,
                   all_segments_dict:dict,
                   train_rule_data_list:list,
                   validation_data_list:list):

    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    input_data_index_list= []
    validation_input_data_list= []
    validation_data_input_index_list= []


    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)
        for train_rule_data_comb in train_rule_data_comb_list:

            data_graph_features= graph_features(train_rule_data_comb,
                                                 train_rule_data_sort,
                                                 expected_data_sort,
                                                 all_segments_dict)



            input_data_list.append(data_graph_features)

            input_data_index_list.append([train_rule_data_sort, train_rule_data_comb])

    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
        for validation_data_comb in validation_data_comb_list:


            data_graph_features= graph_features(validation_data_comb,
                                                 validation_data_sort,
                                                 expected_data_sort,
                                                 all_segments_dict)



            validation_input_data_list.append(data_graph_features)

            validation_data_input_index_list.append([validation_data_sort, validation_data_comb])

    return np.array(input_data_list), input_data_index_list, np.array(validation_input_data_list), validation_data_input_index_list


def get_input_data_with_graph_features_for_general_score(expected_data_sort:list,
                   outside_segment_data_sort:list,
                   all_segments_dict:dict,
                   train_rule_data_list:list,
                   validation_data_list:list):

    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    input_data_index_list= []
    validation_input_data_list= []
    validation_data_input_index_list= []


    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)
        target_data= []
        for train_rule_data_comb in train_rule_data_comb_list:

            data_graph_features= graph_features(train_rule_data_comb,
                                                 train_rule_data_sort,
                                                 expected_data_sort,
                                                 all_segments_dict)


            if len(target_data)==0:
                target_data= data_graph_features
            else:
                target_data+= data_graph_features
        input_data_list.append(target_data)

        input_data_index_list.append([train_rule_data_sort, train_rule_data_comb])

    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
        target_data= []
        for validation_data_comb in validation_data_comb_list:


            data_graph_features= graph_features(validation_data_comb,
                                                 validation_data_sort,
                                                 expected_data_sort,
                                                 all_segments_dict)

            if len(target_data)==0:
                target_data= data_graph_features
            else:
                target_data+= data_graph_features
        validation_input_data_list.append(target_data)

        validation_data_input_index_list.append([validation_data_sort, validation_data_comb])

    return np.array(input_data_list), np.array(validation_input_data_list)







def get_input_data(expected_data_sort:list,
                   outside_segment_data_sort:list,
                   all_segments_dict:dict,
                   train_rule_data_list:list,
                   validation_data_list:list):
    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    input_data_index_list= []
    validation_input_data_list= []
    validation_data_input_index_list= []


    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)
        for train_rule_data_comb in train_rule_data_comb_list:
            first_component= train_rule_data_comb[0]
            second_component= train_rule_data_comb[1]
         #   first_comp_data_dist_info_train_list= data_dist_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
         #   second_comp_data_dist_info_train_list= data_dist_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
         #   data_conn_in_different_level_list= data_conn_in_different_level_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
            first_comp_data_conn_info_train_list, data_component_info_train_list= data_conn_info(first_component,
                                                                                                 train_rule_data_sort,
                                                                                                 expected_data_sort,
                                                                                                 all_segments_dict)
         #   second_comp_data_conn_info_train_list= data_conn_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort, all_segments_dict)

            first_comp_data_posi_info, first_comp_data_posi_cate= data_posi_info(first_component,
                                                                                 train_rule_data_sort,
                                                                                 expected_data_sort,
                                                                                 all_segments_dict)
            second_comp_data_posi_info, second_comp_data_posi_cate= data_posi_info(second_component,
                                                                                   train_rule_data_sort,
                                                                                   expected_data_sort,
                                                                                   all_segments_dict)


            input_data= first_comp_data_conn_info_train_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_cate, first_comp_data_posi_info]+ [second_comp_data_posi_cate, second_comp_data_posi_info]#+ [first_comp_data_in_expected_data_info]+ [second_comp_data_in_expected_data_info]#++ first_comp_binary_features_list+ second_comp_binary_features_list



            input_data_prepro= on_preprocess_one_hot(input_data, len(input_data))

            input_data= input_data_prepro
            input_data_list.append(input_data)


            input_data_index_list.append([train_rule_data_sort, [first_component, second_component]])

    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
        for validation_data_comb in validation_data_comb_list:
            val_data_first_component= validation_data_comb[0]
            val_data_second_component= validation_data_comb[1]

          #  first_comp_data_dist_info_val_list= data_dist_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
          #  second_comp_data_dist_info_val_list= data_dist_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)

         #   all_data_conn_info_list= all_data_conn_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
          #  val_data_conn_in_different_level_list= data_conn_in_different_level_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
            first_comp_data_conn_info_val_list, data_component_info_val_list= data_conn_info(val_data_first_component,
                                                                                             validation_data_sort,
                                                                                             expected_data_sort,
                                                                                             all_segments_dict)
          #  second_comp_data_conn_info_val_list= data_conn_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort, all_segments_dict)

            first_comp_data_posi_info, first_comp_data_posi_cate= data_posi_info(val_data_first_component,
                                                                                 validation_data_sort,
                                                                                 expected_data_sort,
                                                                                 all_segments_dict)
            second_comp_data_posi_info, second_comp_data_posi_cate= data_posi_info(val_data_second_component,
                                                                                   validation_data_sort,
                                                                                   expected_data_sort,
                                                                                   all_segments_dict)

            val_input_data= first_comp_data_conn_info_val_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_cate, first_comp_data_posi_info]+ [second_comp_data_posi_cate, second_comp_data_posi_info]#+ [first_comp_data_in_expected_data_info]+ [second_comp_data_in_expected_data_info]#+ first_comp_binary_features_list+ second_comp_binary_features_list


            val_input_data_prepro= on_preprocess_one_hot(val_input_data, len(val_input_data))
            val_input_data= val_input_data_prepro

            validation_input_data_list.append(val_input_data)

            validation_data_input_index_list.append([validation_data_sort, [val_data_first_component, val_data_second_component]])

    return np.array(input_data_list), input_data_index_list, np.array(validation_input_data_list), validation_data_input_index_list


def get_input_data_0(expected_data_sort, outside_segment_data_sort, all_segments_dict, train_rule_data_list, validation_data_list):
    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    input_data_index_list= []
    validation_input_data_list= []
    validation_data_input_index_list= []


    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)
        for train_rule_data_comb in train_rule_data_comb_list:
            first_component= train_rule_data_comb[0]
            second_component= train_rule_data_comb[1]
         #   first_comp_data_dist_info_train_list= data_dist_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
         #   second_comp_data_dist_info_train_list= data_dist_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
         #   data_conn_in_different_level_list= data_conn_in_different_level_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
            first_comp_data_conn_info_train_list, data_component_info_train_list= data_conn_info(first_component,
                                                                                                 train_rule_data_sort,
                                                                                                 expected_data_sort,
                                                                                                 all_segments_dict)
         #   second_comp_data_conn_info_train_list= data_conn_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort, all_segments_dict)

            first_comp_data_posi_info, first_comp_data_posi_cate= data_posi_info(first_component,
                                                                                 train_rule_data_sort,
                                                                                 expected_data_sort,
                                                                                 all_segments_dict)
            second_comp_data_posi_info, second_comp_data_posi_cate= data_posi_info(second_component,
                                                                                   train_rule_data_sort,
                                                                                   expected_data_sort,
                                                                                   all_segments_dict)


            input_data= first_comp_data_conn_info_train_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_info]+ [second_comp_data_posi_info]#+ [first_comp_data_in_expected_data_info]+ [second_comp_data_in_expected_data_info]#++ first_comp_binary_features_list+ second_comp_binary_features_list

          #   input_data= dist_value_list

            input_data_prepro= on_preprocess_one_hot(input_data, len(input_data))

            input_data= input_data_prepro
            input_data_list.append(input_data)


            input_data_index_list.append([train_rule_data_sort, [first_component, second_component]])

    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
        for validation_data_comb in validation_data_comb_list:
            val_data_first_component= validation_data_comb[0]
            val_data_second_component= validation_data_comb[1]

          #  first_comp_data_dist_info_val_list= data_dist_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
          #  second_comp_data_dist_info_val_list= data_dist_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)

         #   all_data_conn_info_list= all_data_conn_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
          #  val_data_conn_in_different_level_list= data_conn_in_different_level_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
            first_comp_data_conn_info_val_list, data_component_info_val_list= data_conn_info(val_data_first_component,
                                                                                             validation_data_sort,
                                                                                             expected_data_sort,
                                                                                             all_segments_dict)
          #  second_comp_data_conn_info_val_list= data_conn_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort, all_segments_dict)

            first_comp_data_posi_info, first_comp_data_posi_cate= data_posi_info(val_data_first_component,
                                                                                 validation_data_sort,
                                                                                 expected_data_sort,
                                                                                 all_segments_dict)
            second_comp_data_posi_info, second_comp_data_posi_cate= data_posi_info(val_data_second_component,
                                                                                   validation_data_sort,
                                                                                   expected_data_sort,
                                                                                   all_segments_dict)


            val_input_data= first_comp_data_conn_info_val_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_info]+ [second_comp_data_posi_info]#+ [first_comp_data_in_expected_data_info]+ [second_comp_data_in_expected_data_info]#+ first_comp_binary_features_list+ second_comp_binary_features_list

         #   dist_value_list= all_data_dist_info(val_data_first_component, val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)
         #   val_input_data= dist_value_list

            val_input_data_prepro= on_preprocess_one_hot(val_input_data, len(val_input_data))
            val_input_data= val_input_data_prepro

            validation_input_data_list.append(val_input_data)

            validation_data_input_index_list.append([validation_data_sort, [val_data_first_component, val_data_second_component]])

    return input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list



def get_target_layer_output(input_data, model):
    inp = model.input                                           # Input placeholder
    outputs = [layer.output for layer in model.layers]          # All output of layers
    functors = [K.function([inp], [out]) for out in outputs]

    layer_outs = [func([input_data]) for func in functors]      # Get output of target layer that matched the shape of feature of input data
    print ('layer out shape', np.array(layer_outs[-6]).shape)

    return np.squeeze(layer_outs[-6], axis=0)



def get_prob_map(data_list):
    unique, count= np.unique(np.array([i for i in data_list]), return_counts=True)

    unique= np.array([np.around(i, 3) for i in unique])
    count= np.array([i/len(data_list) for i in count])
    mean= round(sum(unique)/len(unique), 3)
    q3= round(np.percentile(unique, 50))

    prob_map= dict(zip(unique, count))
  #  print ('mean', mean)
    return prob_map, q3




def add_corr_features_from_target_layer(input_data, target_layer):
    new_input_data= []
    for i, (data, layer_data) in enumerate(zip(input_data, target_layer)):
        new_data= []
        layer_data_t= np.transpose(layer_data)
        for i_0, (layer_data_feat_t, layer_data_feat) in enumerate(zip(layer_data_t, layer_data)):

            corr, _ = pearsonr(layer_data_feat_t, layer_data_feat)

            target_corr_val=[corr if i_1==1  else 0 for i_1, c in enumerate(np.zeros(len(layer_data_feat)))]

            new_data.append(target_corr_val)

        new_feature_data= data+ np.array(new_data)
        new_input_data.append(new_feature_data)

    return np.array(new_input_data)






def get_input_data_posi_val(expected_data_sort, outside_segment_data_sort, train_rule_data_list, validation_data_list):
    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    input_data_index_list= []
    validation_input_data_list= []
    validation_data_input_index_list= []
    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        for train_rule_data_comb in train_rule_data_sort:
            comp_data_conn_info_list= data_conn_info(train_rule_data_comb, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)
            comp_data_posi_info= data_posi_info(train_rule_data_comb, train_rule_data_sort, expected_data_sort)

            input_data= comp_data_conn_info_list
            input_data.append(comp_data_posi_info)

            input_data_list.append(input_data)


            input_data_index_list.append([train_rule_data_sort, train_rule_data_comb])
    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        for val_data_comb in validation_data_sort:
            comp_data_conn_info_list= data_conn_info(val_data_comb, validation_data_sort, expected_data_sort, outside_segment_data_sort)
            comp_data_posi_info= data_posi_info(val_data_comb, validation_data_sort, expected_data_sort)

            val_input_data= comp_data_conn_info_list
            val_input_data.append(comp_data_posi_info)

            validation_input_data_list.append(val_input_data)
            validation_data_input_index_list.append([validation_data_sort, val_data_comb])

    return input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list

def get_input_data_1(expected_data_sort, train_rule_data_list, validation_data_list):
    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]
    input_data_index_list= []
    validation_input_data_list= []
    validation_data_input_index_list= []
    for train_rule_data in train_rule_data_list:
        train_rule_data_sort= train_rule_data[0]
        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)
        for train_rule_data_comb in train_rule_data_comb_list:
            first_component= train_rule_data_comb[0]
            second_component= train_rule_data_comb[1]

            first_comp_data_dist_info_list= data_dist_info(first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_dist_info_list= data_dist_info(second_component, train_rule_data_sort, expected_data_sort)
            first_comp_data_posi_info= data_posi_info(first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_posi_info= data_posi_info(second_component, train_rule_data_sort, expected_data_sort)


            input_data= train_rule_data_sort+ train_rule_data_comb+ first_comp_data_dist_info_list+ second_comp_data_dist_info_list
            input_data.append(first_comp_data_posi_info)
            input_data.append(second_comp_data_posi_info)
            input_data_list.append(input_data)


            input_data_index_list.append([train_rule_data_sort, [first_component, second_component]])
    for validation_data in validation_data_list:
        validation_data_sort= validation_data[0]
        validation_data_comb_list= sort_to_combination(validation_data_sort)
        for validation_data_comb in validation_data_comb_list:
            val_data_first_component= validation_data_comb[0]
            val_data_second_component= validation_data_comb[1]

            first_comp_data_dist_info_list= data_dist_info(val_data_first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_dist_info_list= data_dist_info(val_data_second_component, train_rule_data_sort, expected_data_sort)
            first_comp_data_posi_info= data_posi_info(val_data_first_component, train_rule_data_sort, expected_data_sort)
            second_comp_data_posi_info= data_posi_info(val_data_second_component, train_rule_data_sort, expected_data_sort)


            val_input_data= validation_data_sort+ validation_data_comb+ first_comp_data_dist_info_list+ second_comp_data_dist_info_list
            val_input_data.append(first_comp_data_posi_info)
            val_input_data.append(second_comp_data_posi_info)


            validation_input_data_list.append(val_input_data)
            validation_data_input_index_list.append([validation_data_sort, [val_data_first_component, val_data_second_component]])

    return input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list

def get_expected_output_data_for_general_score(train_rule_data_list:list)-> np.ndarray:
    max_val, min_val= 5, -8
    data_list= [float(i[-1]) for i in train_rule_data_list]
  #  max_val, min_val= np.max(data_list), np.min(data_list)

    output_value_list= [np.round((i- min_val)/ (max_val- min_val),2)for i in data_list]
    print ('output_value_list', output_value_list)
    return np.array(output_value_list)

def get_expected_output_data(input_data_index_list:list, train_rule_data_list:list, output_list:list)-> np.ndarray:
    all_output_val_list= []
    train_rule_data_index_list= [i[0] for i in train_rule_data_list]
    output_score_list= [float(i[0]) for i in output_list]

    for input_data_index in input_data_index_list:
        target_train_rule_data_index= train_rule_data_index_list.index(input_data_index[0])
        target_train_rule_data= train_rule_data_list[target_train_rule_data_index]

        for component_index in range(len(target_train_rule_data)):
            if component_index== 0:
                continue

            if component_index% 2== 1 and type(target_train_rule_data[component_index])== list:
                if input_data_index[1] == [target_train_rule_data[component_index][0], target_train_rule_data[component_index][2]]:

                    target_output= float(target_train_rule_data[component_index+ 1])
                    norm_target_output= ((target_output -min(output_score_list))/ (max(output_score_list)- min(output_score_list)))

        all_output_val_list.append(norm_target_output)
    return np.array(all_output_val_list)



def get_expected_output_data_posi_val(input_data_index_list, train_rule_data_list, output_list):
    all_output_val_list= []
    train_rule_data_index_list= [i[0] for i in train_rule_data_list]
    for input_data_index in input_data_index_list:
        target_train_rule_data_index= train_rule_data_index_list.index(input_data_index[0])
        target_train_rule_data= train_rule_data_list[target_train_rule_data_index]
        for component_index in range(len(target_train_rule_data)):
            if component_index== 0:
                continue
            if component_index% 2== 1 and type(target_train_rule_data[component_index])== int:
                if input_data_index[1] == target_train_rule_data[component_index]:
                    target_output= target_train_rule_data[component_index+ 1]

                    output_index_list= [i[0] for i in output_list]
                    target_output_index= output_index_list.index(str(target_output))
                    output_val_list= [i[1] for i in output_list]
                    output_val_list[target_output_index]= 1
        all_output_val_list.append(output_val_list)

    return all_output_val_list


def get_expected_output_data_all_data_level(input_data_index_list, train_rule_data_list, output_list):
    all_output_list= []

    train_rule_data_index_list= [i[0] for i in train_rule_data_list]
    for input_data_index in range(len(input_data_index_list)):

        output_val_list= list(np.zeros(len(input_data_index_list)))
        output_val_list[input_data_index]= 1
        all_output_list.append(output_val_list)
    return all_output_list



def init_training_model_for_general_score(expected_data_sort:list,
                  outside_segment_data_sort:list,
                  all_segments_dict:dict,
                  model_archi:str,
                  session_list:list,
                  output_list:list,
                  num_of_layers:int,
                  model,
                  memory_state,
                  carry_state,
                  save_model_dir:str,
                  save_model_for_get_neural_corr_dir:str,
                  checkpoint_dir:str,
                  train_rule_data_dir:str,
                  number_of_units:int,
                  session_index:int,
                  build_new_extracting_features_model:bool):
    tf.keras.backend.clear_session()
    init_train= True
    init_next_layer= False

    save_model_dir=  save_model_dir
    checkpoint_dir= checkpoint_dir



    state= (memory_state, carry_state)
    layer_count= 0

    num_of_layers= num_of_layers


     # Get input, output data
    train_rule_data_list= session_list[session_index][0]
    validation_data_list= session_list[session_index][1]

    input_data_list_arr, validation_input_data_list_arr= get_input_data_with_graph_features_for_general_score(expected_data_sort,
                                                                                            outside_segment_data_sort,
                                                                                            all_segments_dict,
                                                                                            train_rule_data_list,
                                                                                            validation_data_list)
    output_value_list_arr= get_expected_output_data_for_general_score(train_rule_data_list)  # Output expeceted data



    if init_train:
        gc.enable()
        gc.collect()

        print ('train data shape', input_data_list_arr.shape)
        print ('validation data shape', validation_input_data_list_arr.shape)

        batch_size, time_steps, features= input_data_list_arr.shape
        print ('batch', batch_size, 'time_steps', time_steps, 'feature', features)

        #----- Get model for neural correlate of features  -----
        model_for_get_neural_corr= load_model(save_model_for_get_neural_corr_dir)
        if model_for_get_neural_corr:
            if not [time_steps, features]==  model_for_get_neural_corr.input.shape[1:]:
                print ('The model and model for neural corr do not match.')
                model_for_get_neural_corr= False

        if not model_for_get_neural_corr or build_new_extracting_features_model:
            #####  Build model for extracting features  #####
            input_layer_name= 'input_layer_for_add_feature'
            hidden_layer_name= 'hidden_layer_'
            input_layer_for_extract_features= on_input_layer_for_general_score(input_data_list_arr, batch_size, time_steps, features, input_layer_name) # For encoder, decoder

            layer= input_layer_for_extract_features
            init_next_layer= True
            training= False
            while init_next_layer:
                if layer_count== num_of_layers:
                    break
                new_layer, memory_state, carry_state= on_next_layer(layer, layer_count,
                                                                    num_of_layers, state, time_steps,
                                                                    features, number_of_units,
                                                                    model_archi, hidden_layer_name, training, 0)
                state= (memory_state, carry_state)
                layer= new_layer
                layer_count+= 1

            output_layer= on_output_layer_for_general_score(input_data_list_arr, output_value_list_arr, layer, time_steps, features, training)
            model_for_get_neural_corr = tf.keras.Model(inputs= input_layer_for_extract_features, outputs= output_layer, name= 'model_for_extract_features') # Multi
            save_model(model_for_get_neural_corr, save_model_for_get_neural_corr_dir)


        model_for_get_neural_corr.summary()

        #------------------------------------



        if not model:
            #####  Build model  #####

        #    target_layer= get_target_layer_output(input_data_list_arr, model_for_get_neural_corr)

        #    input_data_list_arr= add_corr_features_from_target_layer(input_data_list_arr, target_layer)

         #   target_layer_for_val= get_target_layer_output(validation_input_data_list_arr, model_for_get_neural_corr)
         #   validation_input_data_list_arr= add_corr_features_from_target_layer(validation_input_data_list_arr, target_layer_for_val)
            input_layer_name= 'input_layer'
            hidden_layer_name= 'hidden_layer_'
            batch_size, time_steps, features= input_data_list_arr.shape
            input_layer= on_input_layer_for_general_score(input_data_list_arr, batch_size, time_steps, features, input_layer_name) # For encoder, decoder
            layer_count= 0
            layer= input_layer

            init_next_layer= True
            training= False
            while init_next_layer:
                if layer_count== num_of_layers:
                    break
                new_layer, memory_state, carry_state= on_next_layer(layer, layer_count,
                                                                    num_of_layers, state,
                                                                    time_steps, features,
                                                                    number_of_units, model_archi,
                                                                    hidden_layer_name, training, 0)
                state= (memory_state, carry_state)
                layer= new_layer
                layer_count+= 1

            output_layer= on_output_layer_for_general_score(input_data_list_arr, output_value_list_arr, layer, time_steps, features, training)

            model = tf.keras.Model(inputs= input_layer, outputs= output_layer, name= 'perf_eva_predict_model_for_general_score') # Multi
            model.summary()



        #### Batch  training

        history= train_model(input_data_list_arr, output_value_list_arr, model, checkpoint_dir, batch_size)
      #  model_c= tf.keras.models.clone_model(
     #       model, input_tensors=None, clone_function=None
     #   )



        val_accuracy_list= history.history['val_accuracy']
        accuracy_list= history.history['accuracy']


        init_pred= True




        if init_pred:
            session_index= 1

            train_rule_data_list_for_hist= session_list[session_index][0]
            validation_data_list_for_hist= session_list[session_index][1]

            input_data_list_for_hist_arr, val_data_input_list_for_hist_arr= get_input_data_with_graph_features_for_general_score(expected_data_sort, outside_segment_data_sort, all_segments_dict, train_rule_data_list_for_hist, validation_data_list_for_hist)

          #  target_layer_for_val= get_target_layer_output(val_data_input_list_for_hist_arr, model_for_get_neural_corr)
           # val_data_input_list_for_hist_arr= add_corr_features_from_target_layer(val_data_input_list_for_hist_arr, target_layer_for_val)


            tf.keras.backend.clear_session()
            output_value_list_array= get_expected_output_data_for_general_score(validation_data_list)

            sse, mse= init_predict_on_training_for_general_score(model,
                                                                 val_data_input_list_for_hist_arr,
                                                                 output_value_list_array)
            print ('sse :', sse, 'mse', mse)



            save_model(model, save_model_dir)


        save_model(model, save_model_dir)


def init_training(expected_data_sort:list,
                  outside_segment_data_sort:list,
                  all_segments_dict:dict,
                  model_archi:str,
                  session_list:list,
                  output_list:list,
                  num_of_layers:int,
                  model,
                  memory_state,
                  carry_state,
                  save_model_dir:str,
                  save_model_for_get_neural_corr_dir:str,
                  checkpoint_dir:str,
                  train_rule_data_dir:str,
                  number_of_units:int,
                  session_index:int,
                  build_new_extracting_features_model:bool,
                  add_corr_features:bool):

    tf.keras.backend.clear_session()
    init_train= True
    init_next_layer= False

    save_model_dir=  save_model_dir
    checkpoint_dir= checkpoint_dir




   # tf.keras.utils.set_random_seed(seed)

    state= (memory_state, carry_state)
    layer_count= 0
    model_dropout_rate= 0.1


    num_of_layers= num_of_layers




     # Get input, output data
    train_rule_data_list= session_list[session_index][0]
    validation_data_list= session_list[session_index][1]

    input_data_list_arr, input_data_index_list, validation_input_data_list_arr, validation_data_input_index_list= get_input_data(expected_data_sort, outside_segment_data_sort, all_segments_dict, train_rule_data_list, validation_data_list)
    output_value_list_arr= get_expected_output_data(input_data_index_list, train_rule_data_list, output_list)  # Output expeceted data




    if init_train:
        gc.enable()
        gc.collect()

        print ('length of train data', len(train_rule_data_list))
        print ('length of validation data', len(validation_data_list))




        batch_size, time_steps, features= input_data_list_arr.shape
        print ('batch', batch_size, 'time_steps', time_steps, 'feature', features)

        #----- Get model for neural correlate of features  -----
        model_for_get_neural_corr= load_model(save_model_for_get_neural_corr_dir)
        if model_for_get_neural_corr:
            if not [time_steps, features]==  model_for_get_neural_corr.input.shape[1:]:
                print ('The model and model for neural corr do not match.')
                model_for_get_neural_corr= False

      #  model_for_get_neural_corr= None
        if not model_for_get_neural_corr or build_new_extracting_features_model:
            #####  Build model for extracting features  #####
            input_layer_name= 'input_layer_for_add_feature'
            hidden_layer_name= 'hidden_layer_'
            input_layer_for_extract_features= on_input_layer(input_data_list_arr, batch_size, time_steps, features, input_layer_name) # For encoder, decoder

            layer= input_layer_for_extract_features
            init_next_layer= True
            training= False
            while init_next_layer:
                if layer_count== num_of_layers:
                    break
                new_layer, memory_state, carry_state= on_next_layer(layer, layer_count,
                                                                    num_of_layers, state,
                                                                    time_steps, features,
                                                                    number_of_units, model_archi,
                                                                    hidden_layer_name, training,
                                                                    model_dropout_rate)
                state= (memory_state, carry_state)
                layer= new_layer
                layer_count+= 1

            output_layer= on_output_layer(input_data_list_arr, output_value_list_arr, layer, time_steps, features, training)
            model_for_get_neural_corr = tf.keras.Model(inputs= input_layer_for_extract_features, outputs= output_layer, name= 'model_for_extract_features') # Multi
            save_model(model_for_get_neural_corr, save_model_for_get_neural_corr_dir)

        model_for_get_neural_corr.summary()
        #------------------------------



        if not model:
            #####  Build model  #####
            if add_corr_features:
                target_layer= get_target_layer_output(input_data_list_arr, model_for_get_neural_corr)
                input_data_list_arr= add_corr_features_from_target_layer(input_data_list_arr, target_layer)
                target_layer_for_val= get_target_layer_output(validation_input_data_list_arr, model_for_get_neural_corr)
                validation_input_data_list_arr= add_corr_features_from_target_layer(validation_input_data_list_arr, target_layer_for_val)
            input_layer_name= 'input_layer'
            hidden_layer_name= 'hidden_layer_'
            batch_size, time_steps, features= input_data_list_arr.shape
            input_layer= on_input_layer(input_data_list_arr, batch_size, time_steps, features, input_layer_name) # For encoder, decoder
            layer_count= 0
            layer= input_layer
            training= False
            init_next_layer= True
            while init_next_layer:
                if layer_count== num_of_layers:
                    break
                new_layer, memory_state, carry_state= on_next_layer(layer,
                                                                    layer_count,
                                                                    num_of_layers,
                                                                    state,
                                                                    time_steps,
                                                                    features,
                                                                    number_of_units,
                                                                    model_archi,
                                                                    hidden_layer_name,
                                                                   training,
                                                                    model_dropout_rate)
                state= (memory_state, carry_state)
                layer= new_layer
                layer_count+= 1

            output_layer= on_output_layer(input_data_list_arr, output_value_list_arr, layer, time_steps, features, training)

            model = tf.keras.Model(inputs= input_layer, outputs= output_layer, name= 'perf_eva_predict_model') # Multi
            model.summary()



        #### Batch  training

        history= train_model(input_data_list_arr, output_value_list_arr, model, checkpoint_dir, batch_size)
      #  model_c= tf.keras.models.clone_model(
     #       model, input_tensors=None, clone_function=None
     #   )



        val_accuracy_list= history.history['val_accuracy']
        accuracy_list= history.history['accuracy']




        init_pred= False

        if init_pred:
            session_index= 1

            train_rule_data_list_for_hist= session_list[session_index][0]
            validation_data_list_for_hist= session_list[session_index][1]
            predict_data_list_for_hist= validation_data_list

            input_data_list_for_hist_arr, input_data_index_list_for_hist, val_data_input_list_for_hist_arr, val_data_input_index_list_for_hist= get_input_data(expected_data_sort, outside_segment_data_sort, all_segments_dict, train_rule_data_list_for_hist, validation_data_list_for_hist)
            if  add_corr_features:
                target_layer_for_val= get_target_layer_output(val_data_input_list_for_hist_arr, model_for_get_neural_corr)
                val_data_input_list_for_hist_arr= add_corr_features_from_target_layer(val_data_input_list_for_hist_arr, target_layer_for_val)


            tf.keras.backend.clear_session()


            expected_output_value_list_arr= get_expected_output_data(val_data_input_index_list_for_hist,
                                                                     validation_data_list,
                                                                     output_list)
            sse, mse= init_predict_on_training(model, val_data_input_list_for_hist_arr,
                                               expected_output_value_list_arr,
                                               validation_data_input_index_list,
                                               output_list)



            save_model(model, save_model_dir)


        save_model(model, save_model_dir)













def on_preprocessing_layer_multi_hot(input_data_list, features):
    layer= tf.keras.layers.CategoryEncoding(num_tokens= features, output_mode="multi_hot")
    new_input_data_list= layer(input_data_list)
    #new_input_data_list= tf.keras.layers.Embedding( # Input_dim: Integer. i.e. maximum integer index + 1.
    #  1000,
    #  len(input_data_list),
    #  embeddings_initializer="uniform",
    #  embeddings_regularizer=None,
    #  activity_regularizer=None,
    #  embeddings_constraint=None,
    #  mask_zero=False,
    #  input_length=None,
    #)(input_data_list)
    return new_input_data_list


def on_preprocess_one_hot(input_data_list, data_size):
    #data_range= len(input_data_list)#+ 6
    layer= tf.keras.layers.CategoryEncoding(num_tokens= data_size, output_mode="one_hot")
    new_input_data_list= layer(input_data_list)
    #new_input_data_list= tf.keras.layers.Embedding( # Input_dim: Integer. i.e. maximum integer index + 1.
    #  1000,
    #  len(input_data_list),
    #  embeddings_initializer="uniform",
    #  embeddings_regularizer=None,
    #  activity_regularizer=None,
    #  embeddings_constraint=None,
    #  mask_zero=False,
    #  input_length= None,
    #)(new_input_data_list)

    return new_input_data_list



class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation="sigmoid"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm4 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)



    def call(self, inputs, training:bool):
        attn_output = self.att(inputs, inputs, training= training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)

        out2= self.layernorm2(out1 + ffn_output)

      #  out2_flat= self.flatten(out2)



    #    out_t= tf.transpose(out2_flat, conjugate=False, name='transpose')


      #  corr= tfp.stats.correlation(
      #      out2_flat, out_t, name='correlation', event_axis= None
      #  )


      #  print ('out2 shape', out2.shape, 'corr shape', corr.shape)


      #  out4= self.layernorm4(corr+ out2_flat)


      #  print ('out4', out4.shape)


       # return out4
        return out2

def on_input_layer(input_data_list:list, batch_size:int, time_steps:int, features:int, layer_name:str):
   # input= [samples, time_steps, features] [Number of datas, The length of each data, Each element of the data is a vector of n features]
   # Samples - Number of datas
   # Time steps -   The length of each data
   # Features - Each element of the data is a vector of n features
    input_layer = tf.keras.Input(shape=(time_steps, features), name=layer_name) # shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors.
    input_layer = layers.LayerNormalization(epsilon=1e-6)(input_layer)
    print (input_layer.name)
    return input_layer

def on_input_layer_for_general_score(input_data_list:list, batch_size:int, time_steps:int, features:int, layer_name:str):
   # input= [samples, time_steps, features] [Number of datas, The length of each data, Each element of the data is a vector of n features]
   # Samples - Number of datas
   # Time steps -   The length of each data
   # Features - Each element of the data is a vector of n features
    input_layer = tf.keras.Input(shape=(time_steps, features), name=layer_name) # shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors.
   # input_layer = layers.Softmax(axis=-1)(input_layer)
    input_layer = layers.LayerNormalization(epsilon=1e-6)(input_layer)
    print (input_layer.name)
    return input_layer



def on_next_layer(layer, layer_count:int,
                  num_of_layers:int, state,
                  time_steps, features:int,
                  number_of_units:int, model_archi:str,
                  layer_name:str,
                  training:bool,
                  dropout_rate:float):

    if state[0]== None:
        state= None
    if layer_count== num_of_layers-1:
        dropout= 0
    else:
        dropout= 0

    memory_state, carry_state= None, None

    if model_archi== 'trans':

        transformer_layer= TransformerBlock(features, time_steps, number_of_units, dropout_rate)
        new_layer= transformer_layer(layer, training)

    elif  model_archi== 'lstm':
        lstm= tf.keras.layers.LSTM(units = number_of_units, input_shape= (time_steps, features), name= layer_name+ str(layer_count), return_sequences=True, stateful= False, return_state= True
                                  , time_major= False, activation="tanh", recurrent_activation="sigmoid", unit_forget_bias=True,
                                  kernel_initializer="glorot_uniform", recurrent_initializer="orthogonal", dropout= dropout,
                                  use_bias=True)
        new_layer, memory_state, carry_state= lstm(layer, state)

        if layer_count== num_of_layers-1:


            new_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(features))(new_layer) # TimeDistributed applies the same instance of previous layer to each of the timestamps, the same set of weights are used at each timestamp.#



   # gru= tf.keras.layers.GRU(units= number_of_units, name= 'hidden_layer_'+ str(layer_count), return_sequences=True )



  #  new_layer= gru(new_layer)


    num_heads= int(math.sqrt(number_of_units))


    return new_layer, memory_state, carry_state

def on_output_layer_for_general_score(intput_data_list_arr:np.ndarray, output_value_list_arr:np.ndarray, layer, time_steps:int, features:int, training:bool):

    outputs= tf.keras.layers.Flatten(data_format=None)(layer)
 #   outputs= tf.keras.layers.Dropout(dropout_rate) (outputs, training=training)
    outputs= tf.keras.layers.Dense(units = len(intput_data_list_arr[0]), activation='relu', name='output_dense_layer_0', use_bias=True)(outputs)

    outputs= tf.keras.layers.Dense(units = len(intput_data_list_arr[0]), activation='sigmoid', name='output_dense_layer_1', use_bias=True)(outputs)

    outputs= tf.keras.layers.Dense(units = 1, activation='sigmoid', name='output_dense_layer_2', use_bias=True)(outputs)
    print ('outputs shape', outputs.shape)

    return outputs



def on_output_layer(intput_data_list_arr:np.ndarray, output_value_list_arr:np.ndarray, layer, time_steps:int, features:int, training:bool):

    outputs= tf.keras.layers.Flatten(data_format=None)(layer)
    outputs= tf.keras.layers.Dropout(0.4) (outputs, training=training)
    outputs= tf.keras.layers.Dense(units = len(intput_data_list_arr[0]), activation='relu', name='output_dense_layer_0', use_bias=True)(outputs)

    outputs= tf.keras.layers.Dense(units = len(intput_data_list_arr[0]), activation='sigmoid', name='output_dense_layer_1', use_bias=True)(outputs)

    outputs= tf.keras.layers.Dense(units = 1, activation='sigmoid', name='output_dense_layer_2', use_bias=True)(outputs)
    print ('outputs shape', outputs.shape)

    return outputs



def train_model(input_data_array:np.ndarray, output_data_array:np.ndarray, model, checkpoint_dir:str, batch_size:int):
    optimizer_Adam= tf.keras.optimizers.Adam(learning_rate= 1e-3,
                                              beta_1=0.9,
                                              beta_2=0.999,
                                              epsilon=1e-07,
    )

    rms_prop= tf.keras.optimizers.RMSprop(
        learning_rate=1e-3,
        rho=0.9,
        momentum=0.0,
        epsilon=1e-07,
        centered=False,
        name="RMSprop",
    )

    ada_delta= tf.keras.optimizers.Adadelta(
        learning_rate=1e-3, rho=0.95, epsilon=1e-07, name="Adadelta"
    )

    sgd= tf.keras.optimizers.SGD(learning_rate=1e-3)

    binary_crossentropy= tf.keras.losses.BinaryCrossentropy(
                from_logits= True,
                label_smoothing=0.0,
                axis=-1,
                reduction="auto",
                name="binary_crossentropy",
    )

    categorical_crossentropy= tf.keras.losses.CategoricalCrossentropy(
        from_logits=False,
        label_smoothing=0.0,
        axis=-1,
      #  reduction="auto",
        name="categorical_crossentropy",
    )


    early_stopping= tf.keras.callbacks.EarlyStopping(
            # Stop training when `val_loss` is no longer improving
            monitor="val_loss", # val_loss
            # "No longer improving" being defined as "no better than 1e-2 less"
            min_delta=1e-2,
            # "No longer improving" being further defined as "for at least 2 epochs"
            patience=10,
            verbose=1,
            mode= 'auto'
    )

    callbacks = [
      #  early_stopping,
        tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_dir,
            monitor='val_accuracy',
            mode='auto',
            save_weights_only= True,
            save_best_only=True
        )
    ]


    model.compile(#optimizer= sgd,
              optimizer= optimizer_Adam,
              # Loss function to minimize
              loss= tf.keras.losses.mean_squared_error,
              # List of metrics to monitor
              metrics=['accuracy', 'mse']
            )

    print ('input_data_array shape', input_data_array.shape)
    print ('output_data_array shape', output_data_array.shape)


    print('# Fit model on training data')
    model.summary()
    batch_size= 32
    number_of_epochs= 100

    history = model.fit(input_data_array,
            output_data_array,
        batch_size= batch_size,
        epochs= number_of_epochs,
        validation_split=0.2,
        callbacks= callbacks,
                )

    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

 # results = model.evaluate(validation_input_data_list, validation_expected_output, batch_size=128)
 # print("test loss, test acc:", results)
    return history





def save_model(Model, dir):
    Model.save(
        dir,
      overwrite=True,
      include_optimizer=True,
      save_format=None,
      signatures=None,
      options=None,
      save_traces=True,
    )

def load_model(dir:str):
    return_value= False
    if os.path.exists(dir):
        loaded_model = tf.keras.models.load_model(dir)
        return_value= loaded_model
    else:
        return_value= False

    return return_value

def model_predict(predict_data, model):

    prediction= model.predict(
        predict_data,
        batch_size=None,
        verbose="auto",
        steps=None,
        callbacks=None,
        max_queue_size=10,
        workers=1,
        use_multiprocessing=False,
      )
    return prediction

def init_predict_on_training_for_general_score(model,
                                               predict_input_data_list_arr:np.ndarray,
                                               expected_output_value_list_arr:np.ndarray
                                               ):


    count= 1
    SSE= 0
    for predict_data, expected_output_value in zip(predict_input_data_list_arr,

                                                                       expected_output_value_list_arr):
        detail= None
        SE= 0
        time_steps, features= predict_data.shape
        new_predict_data= predict_data.reshape(1, time_steps, features)
        prediction= model_predict(new_predict_data, model)

        new_predict_data= predict_data.reshape(1, time_steps, features)
        predict_score_val= model_predict(new_predict_data, model)

        predict_score= np.around(predict_score_val*(max(expected_output_value_list_arr)- np.min(expected_output_value_list_arr))+ np.min(expected_output_value_list_arr))
        expected_output_score= expected_output_value*(max(expected_output_value_list_arr)- np.min(expected_output_value_list_arr))+ np.min(expected_output_value_list_arr)

        print (f'predict_score {predict_score} expected_output_score {expected_output_score}')
        SE+= (float(predict_score)- float(expected_output_score))** 2
        if SE!= 0:
            detail= ['predict_score', predict_score, 'expected_score', expected_output_score]
        SSE+= SE
   #   print (predict_data_index, '(predict score- data score)^ 2, SE: ',SE, 'detail: ', detail)

   #   if count% 6 == 0:
   #     print ()
    # print ('predict_score', predict_score )
        count+= 1
    MSE= SSE/ count
    return SSE, MSE

def init_predict_on_training(model, predict_input_data_list_arr:np.ndarray,
                             expected_output_value_list_arr:np.ndarray,
                             predict_input_data_index,
                             output_list:list):



    output_score_list= [float(i[0]) for i in output_list]

    count= 1
    SSE= 0
    for predict_data, predict_data_index, expected_output_value in zip(predict_input_data_list_arr,
                                                                       predict_input_data_index,
                                                                       expected_output_value_list_arr):
        detail= None
        SE= 0

        time_steps, features= predict_data.shape
        new_predict_data= predict_data.reshape(1, time_steps, features)
        prediction= model_predict(new_predict_data, model)

        new_predict_data= predict_data.reshape(1, time_steps, features)
        predict_score_val= model_predict(new_predict_data, model)

        predict_score= np.around(predict_score_val*(max(output_score_list)- min(output_score_list))+ min(output_score_list))
        expected_output_score= expected_output_value*(max(output_score_list)- min(output_score_list))+ min(output_score_list)


        SE+= (float(predict_score)- float(expected_output_score))** 2
        if SE!= 0:
            detail= ['predict_score', predict_score, 'expected_score', expected_output_score]
        SSE+= SE
   #   print (predict_data_index, '(predict score- data score)^ 2, SE: ',SE, 'detail: ', detail)

   #   if count% 6 == 0:
   #     print ()
    # print ('predict_score', predict_score )
        count+= 1
    MSE= SSE/ count
    print ('MSE', MSE)
    return SSE, MSE

def get_pickle_data(target_dir:str):
    if os.path.getsize(target_dir) > 0:

        with open(target_dir, 'rb') as f:

            loaded_dict = pickle.load(f)


        return loaded_dict
    else:
        return {}





def get_ans_info_list(input_ans_list, expected_output, output_steps_records_list):
    correct_ans_info_list= []
    ans_match_list= []
    for ans in input_ans_list[0]:
        if ans in expected_output:
            correct_ans_info= [0, ans[0], ans[1]]
            correct_ans_info_list.append(correct_ans_info)

        if not ans in expected_output:
            ans_info= [1, ans[0], ans[1]]
            ans_match_list.append(ans_info)

    step_probability_list= []
    for i in output_steps_records_list:
        for i_0 in i[2]:
            all_search_in_step= True
            for c in range(len(ans_match_list)):
                if ans_match_list[c] not in i_0:
                    all_search_in_step= False
                else:
                    if not [i[0], i[1],i_0] in step_probability_list:
                        ans_match_list[c].append([i[0], i[1], i_0])
                        step_probability_list.append([i[0], i[1], i_0])


    return ans_match_list, step_probability_list

def get_probility_score(full_score, ans_match_list, step_probability_list, input_ans_index_name):
    total_score= 0

    input_ans_list_2_upload= [] # Input ans 2 upload

    for math_ans_info in ans_match_list:
        #print ('math_ans_info', math_ans_info)
        match_ans_probability= math_ans_info[3][1]
        match_ans_info_list= math_ans_info[3][2]

        match_ans_score=  1/ int(match_ans_probability)* full_score

        print (math_ans_info[1], '->', math_ans_info[2], 'wrong answer: get score '+ str(round(match_ans_score, 1)), 'probability '+ str(1)+ '/', int(match_ans_probability))
        total_score+= match_ans_score

        # Ans info to dataset [src_id, target_id, src_label, target_label, description]
        src_id= math_ans_info[1]
        target_id= math_ans_info[2]
        arrow= 'to' # attribute 'from', 'to
        src_label= input_ans_index_name[math_ans_info[1]]
        description= 'wrong answer: get score '+ str(round(match_ans_score, 1))
        target_label= input_ans_index_name[math_ans_info[2]]
        color= 'rgb(255, 0, 0)'
        input_ans_list_2_upload.append([src_id, target_id, src_label, target_label, description, color])

    #print ('total_score', total_score)
    return total_score, input_ans_list_2_upload

def get_output_steps_records_list(dir):
    with open(dir, 'r', newline='') as csvfile:
        content= []
        reader = csv.reader(csvfile, delimiter=',')
        for row in reader:
            sub_content= []
            sub_content.append(row[0])
            sub_content.append(int(row[1]))
            step_record= ast.literal_eval(row[2])
            sub_content.append(step_record)
            content.append(sub_content)

    return content


# Model param
memory_state= None
carry_state= None

model_archi_dict= {0: 'lstm', 1: 'trans'}
model_archi= model_archi_dict.get(1)

if model_archi== 'trans':
    number_of_layers= 1
else:
    number_of_layers= 6

number_of_units= 256

#model= load_model(save_model_dir)
model= False
build_new_extracting_features_model= True

variance_dict={}
mse_dict= {}

# Training process var

train_rule_data_folder_dir= os.path.join(root_dir, 'target_train_data')
if [i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv"))]:
    train_rule_data_dir= [i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv"))][-1]
    print (f'Target train data {train_rule_data_dir}')
    target_q_id= int(os.path.splitext(os.path.basename(train_rule_data_dir))[0].split('_')[3].replace('q', ''))
dir= root_dir
#train_rule_data_name= 'train_rule_data_for_con_demo_man.csv'
#train_rule_data_dir= os.path.join(dir, train_rule_data_name)
train_and_val_data_name= 'train_data_for_val.csv'
save_train_and_val_data_dir= os.path.join(root_dir, train_and_val_data_name)
train_general_score_model= False
seed= 8
tf.keras.utils.set_random_seed(seed)
# Data to features var

target_correct_ans_list= get_pickle_data(os.path.join(root_dir, 'correct_ans_dict.pkl'))[target_q_id]
expected_data_sort= (np.array(target_correct_ans_list)-1).tolist()
number_of_outside_number_of_segments= 6
outside_segment_data_sort=[i for i in range(14) if not i in expected_data_sort]+ [201]

all_segments_data_sort= expected_data_sort+ outside_segment_data_sort

all_segments_dict= dict()
for segment_index, segment in enumerate(all_segments_data_sort):
    in_exepcted_data= 0
    if segment in expected_data_sort:
        in_exepcted_data= 1
    all_segments_dict[segment]= str(in_exepcted_data)+ '_'+ str(segment_index) # {In expected data} _ {category of segment}

print ('all_segments_dict', len(all_segments_dict), all_segments_dict)

output_list= []
[output_list.append([str(score), 0]) for score in range(-5, 6)]


# Save model dir
task_name= 'for_report'#'_for_conference' #_for_conference_with_test_model
save_model_dir= os.path.join(dir, f'perf_eva_predict_model_{number_of_layers}_layers_{task_name}' )
save_model_for_get_neural_corr_dir=os.path.join(dir, f'model_for_extract_features_{model_archi}' )
checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')


if not os.path.exists(save_model_dir):
    os.mkdir(save_model_dir)
if not os.path.exists(checkpoint_dir):
    os.mkdir(checkpoint_dir)



# Set train data var
train_with_classes= False
session_index= 2
if session_index==0:
    gen_new_session_list= True
else:
    gen_new_session_list= False

add_corr_features = True
print (f'session_index {session_index}')


# Get train val data by different classes
if train_with_classes:
    target_question= 5
    train_rule_data_folder_dir= os.path.join(root_dir, 'target_train_data_with_classes')
    session_list_with_classes= get_train_and_val_data_by_classes([i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv")) if
                                                                  f'q{target_question}' in i])


# Get session and save train and validation data

if not os.path.exists(save_train_and_val_data_dir) or gen_new_session_list:
    if train_general_score_model:
        session_list= get_train_and_val_data_for_general_score(train_rule_data_dir)
        save_train_and_val_data(save_train_and_val_data_dir,session_list )

    else:
        #session_list= get_cross_train_rule_data_and_validation_data(target_dir)

   # session_list= get_target_train_rule_data_and_validation_data_test_data(train_rule_data_dir)
        session_list= get_train_and_val_data_by_split(train_rule_data_dir)
        save_train_and_val_data(save_train_and_val_data_dir,session_list )

else:
    with open(save_train_and_val_data_dir, 'r+', newline='') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')

        session_list= [[ast.literal_eval(i_0) for i_0 in i] for i in reader]




start_train= True
if start_train:
    if train_general_score_model:
        number_of_units= 16
        save_model_dir= os.path.join(dir, f'perf_eva_predict_model_{number_of_layers}_layers_{task_name}_for_general_score' )
        save_model_for_get_neural_corr_dir=os.path.join(dir, f'model_for_extract_features_{model_archi}_for_general_score' )
        checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')

        session_list= get_train_and_val_data_for_general_score(train_rule_data_dir)


        build_new_extracting_features_model= False


        init_training_model_for_general_score(expected_data_sort, outside_segment_data_sort, all_segments_dict, model_archi
              , session_list, output_list, number_of_layers, model , memory_state, carry_state
              , save_model_dir, save_model_for_get_neural_corr_dir, checkpoint_dir
              , train_rule_data_dir, number_of_units,
              session_index,
              build_new_extracting_features_model)


    elif train_with_classes:
        target_session_list_index= 0
        session_list= session_list_with_classes[target_session_list_index]

        build_new_extracting_features_model= False


        init_training(expected_data_sort, outside_segment_data_sort, all_segments_dict, model_archi,
              session_list, output_list, number_of_layers, model , memory_state, carry_state,
              save_model_dir, save_model_for_get_neural_corr_dir, checkpoint_dir,
              train_rule_data_dir, number_of_units,
              session_index,
              build_new_extracting_features_model,
              add_corr_features)

    else:


        build_new_extracting_features_model= False


        init_training(expected_data_sort, outside_segment_data_sort, all_segments_dict, model_archi,
              session_list, output_list, number_of_layers, model , memory_state, carry_state,
              save_model_dir, save_model_for_get_neural_corr_dir, checkpoint_dir,
              train_rule_data_dir, number_of_units,
              session_index,
              build_new_extracting_features_model,
              add_corr_features)


#weights = model.get_weights() # Getting params
#model.set_weights(weights) # Setting params
#weights = model.layers[i].get_weights() # Getting params
#model.layers[i].set_weights(weights) # Setting params

import os




def load_model(dir):
    loaded_model = tf.keras.models.load_model(dir)
    return loaded_model


def model_predict(predict_data, model):

    prediction= model.predict(
        predict_data,
        batch_size=None,
        verbose="auto",
        steps=None,
        callbacks=None,
        max_queue_size=10,
        workers=1,
        use_multiprocessing=False,
    )
    return prediction

def save_predict_data(target_dir, data):
    with open(target_dir, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(data)


def save_score_list(target_dir, data):
    with open(target_dir, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(data)



def init_predict(dir:str, model, all_segments:list, predict_input_data_list_arr, predict_input_data_index, predict_data_list, output_list):



    expected_output_value_list= get_expected_output_data(predict_input_data_index, predict_data_list, output_list)
    output_score_list= [float(i[0]) for i in output_list]

    count= 1
    SSE= 0
    save_predict_data_list= []
    total_score= 0
    total_score_list= []
    for predict_data, predict_data_index, expected_output_value in zip(predict_input_data_list_arr, predict_input_data_index, expected_output_value_list):

        # Pass if empty option in predict dataset
        if 13 in predict_data_index[1]:
            continue
        detail= 'Match'
        SE= 0
        time_steps, features= predict_data.shape
        new_predict_data= predict_data.reshape(1, time_steps, features)
        predict_score_val= model_predict(new_predict_data, model)


        predict_score= np.around(predict_score_val*(max(output_score_list)- min(output_score_list))+ min(output_score_list))
        expected_output_score= expected_output_value*(max(output_score_list)- min(output_score_list))+ min(output_score_list)


        SE+= (float(predict_score)- float(expected_output_score))** 2
        if SE!= 0:
            detail= ['predict_score', predict_score, 'expected_score', expected_output_score]
        else:
            detail= ['Match  ', 'predict_score', predict_score, 'expected_score', expected_output_score]
        SSE+= SE
        print (predict_data_index, '(predict score- data score)^ 2, SE: ',SE, 'detail: ', detail)
        data_row= [predict_data_index, '(predict score- data score)^ 2, SE: ',SE, 'detail: ', detail]
        save_predict_data_list.append(data_row)
        total_score+= float(predict_score)
        if count% (int(len(predict_data_list[0][0]))- 1) == 0:
            save_predict_data_list.append([])
            total_score_list.append([predict_data_index[0], total_score])
            total_score= 0
            print ()

        count+= 1

        # Add expected value to variance dict
        if  not f'{predict_data_index[1][0]}_{predict_data_index[1][1]}' in mse_dict:
            mse_dict[f'{predict_data_index[1][0]}_{predict_data_index[1][1]}']= [SE]
        else:
            mse_dict[f'{predict_data_index[1][0]}_{predict_data_index[1][1]}'].append(SE)


    MSE= np.round(SSE/ count,2)

    print ('MSE', MSE)

    predict_data_dir= os.path.join(dir, 'predict_data.csv')
    predict_score_data_dir= os.path.join(dir, 'predict_score_data.csv')
    save_predict_data(predict_data_dir, save_predict_data_list)
    save_score_list(predict_score_data_dir, total_score_list)

    return MSE

def get_variance_plot(variance_dict:dict, mse_dict:dict):
    x_list= []
    y_list= []
    for i in variance_dict:
        if not i in mse_dict or not len(variance_dict[i])>1:
            continue

        variance_val= statistics.variance(variance_dict[i])
        mse_val= np.max(mse_dict[i])
        x_list.append(variance_val)
        y_list.append(mse_val)


    print (x_list)

    plt.plot(  x_list, y_list, 'ro')
    plt.axis((np.min(x_list), np.max(x_list), np.min(y_list), np.max(y_list)))
    plt.show()



start_predict= True
if start_predict:

    dir= root_dir
    save_model_dir= os.path.join(dir, f'perf_eva_predict_model_{number_of_layers}_layers_{task_name}' )

    model= load_model(save_model_dir)
    model.summary()
    get_variance_data(session_list, session_index)
    session_index= 1

    train_rule_data_list, validation_data_list= session_list[session_index][0], session_list[session_index][1]

    predict_data_list= validation_data_list

    input_data_list_arr, input_data_index_list, val_input_data_list_arr, val_input_data_index_list= get_input_data(expected_data_sort, outside_segment_data_sort, all_segments_dict, train_rule_data_list, validation_data_list)

    if  add_corr_features:
        model_for_get_neural_corr= load_model(save_model_for_get_neural_corr_dir)
        target_layer_for_val= get_target_layer_output(val_input_data_list_arr, model_for_get_neural_corr)
        val_input_data_list_arr= add_corr_features_from_target_layer(val_input_data_list_arr, target_layer_for_val)

    init_predict(dir, model, expected_data_sort, val_input_data_list_arr, val_input_data_index_list, predict_data_list, output_list)

    get_variance_plot(variance_dict, mse_dict)
    device = cuda.get_current_device()
    device.reset()

with_add_corr_feature= 1.63,
without_add_corr_feature= 2.09, 4.48

def init_sequence(question_name:str, r_index:int, train_session_index:int, session_list:list):
        dir= root_dir

        session_index= train_session_index
        if train_general_score_model:
            save_model_dir= os.path.join(dir, f'perf_eva_predict_model_{number_of_layers}_layers_{task_name}_for_general_score' )
            save_model_for_get_neural_corr_dir=os.path.join(dir, f'model_for_extract_features_{model_archi}_for_general_score' )
            checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')
            build_new_extracting_features_model= False
            init_training_model_for_general_score(expected_data_sort, outside_segment_data_sort, all_segments_dict, model_archi
                      , session_list, output_list, number_of_layers, model , memory_state, carry_state
                      , save_model_dir, save_model_for_get_neural_corr_dir, checkpoint_dir
                      , train_rule_data_dir, number_of_units,
                      session_index,
                      build_new_extracting_features_model)


        else:
            # Save model dir
            task_name= 'for_report'#'_for_conference' #_for_conference_with_test_model
            save_model_dir= os.path.join(dir, f'perf_eva_predict_model_{number_of_layers}_layers_{task_name}' )
            save_model_for_get_neural_corr_dir=os.path.join(dir, f'model_for_extract_features_{model_archi}' )
            checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')
            model= False


            build_new_extracting_features_model= False

            init_training(expected_data_sort, outside_segment_data_sort, all_segments_dict
                      , model_archi, session_list, output_list, number_of_layers, model
                      , memory_state, carry_state, save_model_dir, save_model_for_get_neural_corr_dir
                      , checkpoint_dir, train_rule_data_dir, number_of_units, session_index,
                      build_new_extracting_features_model,
                     add_corr_features)

            # Init predict

            val_session_index= 1

            train_rule_data_list= session_list[val_session_index][0]
            validation_data_list= session_list[val_session_index][1]
            predict_data_list= validation_data_list

            input_data_list_arr, input_data_index_list, val_input_data_list_arr, val_input_data_index_list= get_input_data(expected_data_sort, outside_segment_data_sort, all_segments_dict, train_rule_data_list, validation_data_list)

            if  add_corr_features:
                model_for_get_neural_corr= load_model(save_model_for_get_neural_corr_dir)
                target_layer_for_val= get_target_layer_output(val_input_data_list_arr, model_for_get_neural_corr)
                val_input_data_list_arr= add_corr_features_from_target_layer(val_input_data_list_arr, target_layer_for_val)
            trained_model= load_model(save_model_dir)

            mse= init_predict(dir, trained_model,
                         expected_data_sort,
                         val_input_data_list_arr,
                         val_input_data_index_list,
                         predict_data_list,
                         output_list)
            if not r_index in data_dict[question_name]:
                data_dict[question_name][r_index]= {len(session_list[train_session_index][0]): mse}
            else:
                data_dict[question_name][r_index][len(session_list[train_session_index][0])]= mse

def init_sequence_with_classes(question_name:str, r_index:int, train_session_index:int, session_list:list, val_session_list:list):
        dir= root_dir

        session_index= train_session_index
        if train_general_score_model:
            save_model_dir= os.path.join(dir, f'perf_eva_predict_model_{number_of_layers}_layers_{task_name}_for_general_score' )
            save_model_for_get_neural_corr_dir=os.path.join(dir, f'model_for_extract_features_{model_archi}_for_general_score' )
            checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')
            build_new_extracting_features_model= False
            init_training_model_for_general_score(expected_data_sort, outside_segment_data_sort, all_segments_dict, model_archi
                      , session_list, output_list, number_of_layers, model , memory_state, carry_state
                      , save_model_dir, save_model_for_get_neural_corr_dir, checkpoint_dir
                      , train_rule_data_dir, number_of_units,
                      session_index,
                      build_new_extracting_features_model)


        else:
            # Save model dir
            task_name= 'for_report'#'_for_conference' #_for_conference_with_test_model
            save_model_dir= os.path.join(dir, f'perf_eva_predict_model_{number_of_layers}_layers_{task_name}' )
            save_model_for_get_neural_corr_dir=os.path.join(dir, f'model_for_extract_features_{model_archi}' )
            checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')
            model= False

            build_new_extracting_features_model= False

            init_training(expected_data_sort, outside_segment_data_sort, all_segments_dict
                      , model_archi, session_list, output_list, number_of_layers, model
                      , memory_state, carry_state, save_model_dir, save_model_for_get_neural_corr_dir
                      , checkpoint_dir, train_rule_data_dir, number_of_units, session_index,
                      build_new_extracting_features_model,
                     add_corr_features)

            # Init predict

            train_rule_data_list= val_session_list[-1][0]
            validation_data_list= val_session_list[-1][1]
            print ('validation_data_list len', len(validation_data_list))
            predict_data_list= validation_data_list

            input_data_list_arr, input_data_index_list, val_input_data_list_arr, val_input_data_index_list= get_input_data(expected_data_sort, outside_segment_data_sort, all_segments_dict, train_rule_data_list, validation_data_list)

            if  add_corr_features:
                model_for_get_neural_corr= load_model(save_model_for_get_neural_corr_dir)
                target_layer_for_val= get_target_layer_output(val_input_data_list_arr, model_for_get_neural_corr)
                val_input_data_list_arr= add_corr_features_from_target_layer(val_input_data_list_arr, target_layer_for_val)
            trained_model= load_model(save_model_dir)

            mse= init_predict(dir, trained_model,
                         expected_data_sort,
                         val_input_data_list_arr,
                         val_input_data_index_list,
                         predict_data_list,
                         output_list)
            data_dict[question_name][len(session_list[train_session_index][0])]= mse

def process_all_data():
    t= None
    if [i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv"))]:

        for i, c in enumerate([i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv"))]):

            train_rule_data_dir= c
            print (f'Target train data {train_rule_data_dir}')
            target_q_name= os.path.splitext(os.path.basename(train_rule_data_dir))[0].split('_')[4]
            target_q_id= int(os.path.splitext(os.path.basename(train_rule_data_dir))[0].split('_')[3].replace('q', ''))
            if target_q_id in [2, 3, 4]:
                continue
            data_dict[f'{target_q_id}_{target_q_name}']= {}
            session_list= get_train_and_val_data_by_split(train_rule_data_dir)

            for  r_seed_index  in range(7, 12):
                tf.keras.utils.set_random_seed(r_seed_index)

                session_count= 0
                while session_count< len(session_list)-1:
                    print ('r_seed_index', r_seed_index, 'session_count', session_count)


                    if t== None or not t.is_alive():

                        t = Thread(target=init_sequence, args=[f'{target_q_id}_{target_q_name}', r_seed_index, session_count, session_list])
                        t.start()
                        session_count+= 1
                        print ('data_dict', data_dict)

                    if  t.is_alive():
                        time.sleep(30) # 30

                    else:

                        time.sleep(5)
                        gc.enable()
                        gc.collect()
                        device = cuda.get_current_device()
                        device.reset()

                        time.sleep(5)
                        if r_seed_index>= 9:
                            session_count= len(session_list)
                            break

    get_variance_plot(variance_dict, mse_dict)
    with open(os.path.join(root_dir, 'data_dict.pkl'), 'wb') as f:
            pickle.dump(data_dict, f)


def process_all_data_with_classes():
    t= None
    if [i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv"))]:
        target_val_class_index= 0
        val_data_dir= [i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv"))][target_val_class_index]
        print (f'Target val data {val_data_dir}')
        for i, c in enumerate([i for i in glob.iglob(os.path.join(train_rule_data_folder_dir, "*.csv"))]):
            train_rule_data_dir= c
            if i== target_val_class_index:

                continue
            print (f'Target train data {train_rule_data_dir}')
            target_q_name= os.path.splitext(os.path.basename(train_rule_data_dir))[0].split('_')[4]
            target_q_id= int(os.path.splitext(os.path.basename(train_rule_data_dir))[0].split('_')[3].replace('q', ''))
            train_class= os.path.splitext(os.path.basename(train_rule_data_dir))[0].split('_')[5]
            val_class= os.path.splitext(os.path.basename(val_data_dir))[0].split('_')[5]
            target_index= f'{train_class}->{val_class}'
            data_dict[target_index]= {}

            r_seed_index= 9
            session_list= get_train_and_val_data_by_split(train_rule_data_dir)
            val_session_list= get_train_and_val_data_by_split(val_data_dir)

            print ('r_seed_index', r_seed_index)
            tf.keras.utils.set_random_seed(r_seed_index)
            session_index= 0
            while session_index in range(len(session_list)+ 1) :



                if t== None or not t.is_alive()  :
                    print ('session_index', session_index)
                    if session_index > len(session_list)-1:
                        session_index= len(session_list)+ 1
                    t = Thread(target=init_sequence_with_classes, args=[target_index, r_seed_index,
                                                                        session_index, session_list, val_session_list])
                    t.start()

                    session_index+= 1
                    print ('data_dict', data_dict)

                if  t.is_alive():
                        time.sleep(30) # 30

                else:

                        time.sleep(5)
                        gc.enable()
                        gc.collect()
                        device = cuda.get_current_device()
                        device.reset()

                        time.sleep(5)






  #  get_variance_plot(variance_dict, mse_dict)
    with open(os.path.join(root_dir, 'data_dict.pkl'), 'wb') as f:
            pickle.dump(data_dict, f)



data_dict= {}
if train_with_classes:

    train_rule_data_folder_dir= os.path.join(root_dir, 'target_train_data_with_classes')
else:
    train_rule_data_folder_dir= os.path.join(root_dir, 'target_train_data')
if not train_with_classes:
    process_all_data()
else:
    process_all_data_with_classes()

q5= {'post->med': {20: 13.5, 30: 8.62, 40: 5.14, 43: 4.08}, 'phar->med': {20: 8.95, 30: 9.03, 40: 2.64, 51: 3.41}}
{'med->post': {20: 2.95, 30: 2.04, 40: 2.06, 61: 1.79}, 'phar->post': {20: 9.06, 30: 9.82, 40: 4.47, 51: 4.32}}
 {'med->phar': {20: 3.94, 30: 3.19, 40: 2.97, 61: 1.81}, 'post->phar': {20: 3.97, 30: 3.51, 40: 2.65, 43: 2.51}}

#print (data_dict)

q2= {7: {20: 4.07, 30: 4.79, 40: 3.55}, 8: {20: 2.13, 30: 2.41, 40: 1.93}, 9: {20: 5.48, 30: 3.73, 40: 0.86}}
q3= {7: {20: 4.2, 30: 3.85, 40: 6.47}, 8: {20: 5.68, 30: 4.57, 40: 2.1}, 9: {20: 2.75, 30: 8.46, 40: 2.35}}
q4= {7: {20: 3.42, 30: 2.91, 40: 2.23}, 8: {20: 13.03, 30: 8.85, 40: 5.8}, 9: {20: 7.22, 30: 5.56, 40: 4.57}, 10: {20: 4.37, 30: 2.3, 40: 1.64}
q5= {7: {20: 4.41, 30: 3.46, 40: 2.78}, 8: {20: 10.9, 30: 9.92, 40: 8.16}, 9: {20: 8.69, 30: 2.8, 40: 1.57}}
q7= {7: {20: 2.91, 30: 3.6, 40: 2.22}, 8: {20: 4.29, 30: 3.82, 40: 4.35}, 9: {20: 4.74, 30: 4.15, 40: 2.07}}



target_dict_list= [q2, q3, q4, q5, q7]
for target_dict in target_dict_list:
    print (f'{target_dict=}'.split('=')[0])

    for i in target_dict:
        print (f'In random order {i} {[[i0,target_dict[i][i0]] for i0 in target_dict[i]]}')

# q2 param, number_of_train_data= 20, number_of_val_data= 15, mse=2.74
# q2 param, number_of_train_data= 30, number_of_val_data=15, mse= 5.29
# q2 param, number_of_train_data= 40, number_of_val_data=15, mse= 6.09

#q4 param, number_of_train_data= 20, number_of_val_data= 15, mse=3.1
#q4 param, number_of_train_data= 30, number_of_val_data=15, mse= 1.78

#SSE 71
#MSE 2.7

x = tf.random.normal(shape=(100, 2, 3))
y = tf.random.normal(shape=(100, 2, 3))

# corr[i, j] is the sample correlation between x[:, i, j] and y[:, i, j].
corr = tfp.stats.correlation(x, y, sample_axis=0, event_axis=None)

# corr_matrix[i, m, n] is the sample correlation of x[:, i, m] and y[:, i, n]
corr_matrix = tfp.stats.correlation(x, y, sample_axis=0, event_axis=-1)

print (corr)
print (corr_matrix)